{"cells":[{"cell_type":"code","source":["# import libraries\n","\n","from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import col, lit, count, when, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n","from datetime import datetime\n","from delta.tables import DeltaTable\n","import os\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.2403227Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:07.2420475Z","execution_finish_time":"2025-03-18T20:31:07.5935044Z","parent_msg_id":"cab92413-26e2-4fdd-8b6d-2b94a4b1373a"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 67, Finished, Available, Finished)"},"metadata":{}}],"execution_count":65,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8181829a-5853-43f4-bb3a-56c7f49cd74f"},{"cell_type":"code","source":["# initialize session\n","spark = SparkSession.builder \\\n","    .appName(\"run_materialization\") \\\n","    .getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":68,"statement_ids":[68],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.2741628Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:07.5964473Z","execution_finish_time":"2025-03-18T20:31:07.9177239Z","parent_msg_id":"92fe951b-a845-4055-a7a0-8717b1b280fe"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 68, Finished, Available, Finished)"},"metadata":{}}],"execution_count":66,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89bd6d73-d177-488e-9349-93efe827c96c"},{"cell_type":"markdown","source":["#### Input parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd080a02-7e6a-4818-a1fc-6a1534fce8cd"},{"cell_type":"code","source":["workspace = 'BUNN_Foundation_NONPROD'\n","lakehouse = 'silver_sapecc_lakehouse'\n","inputsourceschema = 'materialized_etl'\n","inputsource = 'orders_etl'\n","outputtargetschema = 'materialized_t'\n","outputtarget = 'orders'\n","update_control_table = 0\n","load_option = 'CLDUI' \n","run_start = datetime.now()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":69,"statement_ids":[69],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.3301992Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:07.9205355Z","execution_finish_time":"2025-03-18T20:31:08.205422Z","parent_msg_id":"946d6c40-13a9-4060-991a-1782ddc09efc"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 69, Finished, Available, Finished)"},"metadata":{}}],"execution_count":67,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false},"tags":["parameters"]},"id":"925f5d41-5227-4b5c-8fb0-e71ab2d767d7"},{"cell_type":"code","source":["# Begin materialization process\n","\n","spark.sql(f\"\"\"\n","INSERT INTO materialization_log (schema_name, table_name, job_run_timestamp, run_id, run_step, run_timestamp, statement_text, lakehouse_name)\n","VALUES ('{inputsourceschema}', '{inputsource}', '{run_start}', 10, 'begin materialization', current_timestamp(), null, '{lakehouse}')\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":70,"statement_ids":[70],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.4758362Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:08.2079821Z","execution_finish_time":"2025-03-18T20:31:11.5098223Z","parent_msg_id":"265cb420-ec43-4767-8b7a-4111027eeb8a"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 70, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":209,"data":{"text/plain":"DataFrame[]"},"metadata":{}}],"execution_count":68,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9e4174a-5a1f-4419-967d-f3293468c59f"},{"cell_type":"code","source":["# insert/update materialization control table\n","\n","if update_control_table == 1:\n","    # update materialization_control if update_control_table = 1\n","    query = f\"\"\"\n","    UPDATE materialization_control\n","    SET control_start_timestamp = current_timestamp()\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND lakehouse_name = '{lakehouse}'\n","    AND load_option = '{load_option}'\n","    \"\"\"\n","    spark.sql(query)    \n","else:\n","    # insert into materialization_control if no matching record exists\n","    query = f\"\"\"\n","    INSERT INTO materialization_control (etl_database, etl_view, status_code, load_option, extract_start_timestamp, control_start_timestamp, lakehouse_name)\n","    SELECT '{inputsourceschema}', '{inputsource}', 'active', '{load_option}', '{run_start}', null, '{lakehouse}'\n","    WHERE NOT EXISTS (\n","        SELECT 1\n","        FROM materialization_control\n","        WHERE etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    )\n","    \"\"\"\n","    spark.sql(query)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":71,"statement_ids":[71],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.5621729Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:11.5125646Z","execution_finish_time":"2025-03-18T20:31:14.8776493Z","parent_msg_id":"a2a51e4f-275d-421e-b096-741289379ec3"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 71, Finished, Available, Finished)"},"metadata":{}}],"execution_count":69,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cebc1052-0ad5-45ae-8e62-f3b72d92f122"},{"cell_type":"code","source":["# Update materialization load table execution times\n","\n","spark.sql(f\"\"\"\n","update materialization_load\n","    set last_execution = current_timestamp\n","    WHERE 1=1\n","    and lakehouse_name = '{lakehouse}'\n","    and input_source_schema = '{inputsourceschema}'\n","    and input_source = '{inputsource}'\n","    and output_target_schema = '{outputtargetschema}'\n","    and output_target = '{outputtarget}'\n","    and load_option = '{load_option}'\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":72,"statement_ids":[72],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.6431726Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:14.8805567Z","execution_finish_time":"2025-03-18T20:31:15.6565135Z","parent_msg_id":"b541b46b-316e-44a5-a066-5e7144ac9bcb"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 72, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":215,"data":{"text/plain":"DataFrame[num_affected_rows: bigint]"},"metadata":{}}],"execution_count":70,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ad19741-54a0-42f3-9b6f-f6dcf83a5317"},{"cell_type":"markdown","source":["#### Metadata checks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6ffc79e-f16a-4057-83b4-10f8bc62c567"},{"cell_type":"code","source":["# define input/output table path\n","\n","source_table_path  = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{inputsourceschema}/{inputsource}\"\n","target_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{outputtargetschema}/{outputtarget}\"\n","log_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/utilities_lakehouse.Lakehouse/Tables/materialization_log\"\n","\n","#print(source_schema)\n","#df = spark.read.format(\"delta\").load(output_table)\n","#df.createOrReplaceTempView(\"orders\")\n","#display(df.head(5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":73,"statement_ids":[73],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.8460991Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:15.6592114Z","execution_finish_time":"2025-03-18T20:31:15.9601909Z","parent_msg_id":"0bdc9ee5-60b5-460c-9a6d-f0c35b7344bc"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 73, Finished, Available, Finished)"},"metadata":{}}],"execution_count":71,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"10041612-bf85-4617-bc91-1bcc6f9cc607"},{"cell_type":"code","source":["# define log schema explicitly\n","\n","log_schema = StructType([\n","    StructField(\"lakehouse_name\", StringType(), True),\n","    StructField(\"schema_name\", StringType(), True),\n","    StructField(\"table_name\", StringType(), True),\n","    StructField(\"job_run_timestamp\", TimestampType(), True),\n","    StructField(\"run_id\", IntegerType(), True), \n","    StructField(\"run_step\", StringType(), True),\n","    StructField(\"run_timestamp\", TimestampType(), True),\n","    StructField(\"record_count\", IntegerType(), True),\n","    StructField(\"step_fail\", StringType(), True),\n","    StructField(\"statement_text\", StringType(), True)\n","])\n","\n","# insert log function\n","\n","def insert_log(lakehouse_name, schema_name, table_name, job_run_timestamp,\n","             run_id, run_step, run_timestamp, record_count, step_fail, statement_text):\n","    log_data = [\n","        Row(\n","            lakehouse_name=lakehouse_name,\n","            schema_name=schema_name,\n","            table_name=table_name,\n","            job_run_timestamp=job_run_timestamp,\n","            run_id=run_id,\n","            run_step=run_step,\n","            run_timestamp=run_timestamp,\n","            record_count = record_count,\n","            step_fail=step_fail,\n","            statement_text=statement_text\n","        )\n","    ]\n","    log_df = spark.createDataFrame(log_data, schema=log_schema)\n","    log_df.write.format(\"delta\").mode(\"append\").save(log_table_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":74,"statement_ids":[74],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:07.9138814Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:15.962712Z","execution_finish_time":"2025-03-18T20:31:16.2431651Z","parent_msg_id":"41be5260-a15f-449f-a807-76c0aeae7bed"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 74, Finished, Available, Finished)"},"metadata":{}}],"execution_count":72,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea736ca4-1f58-407e-b5f3-3ff6af8a9b84"},{"cell_type":"code","source":["#1 Check if the source table exist\n","try:\n","    if not mssparkutils.fs.exists(source_table_path):\n","         raise Exception(f\"Source table {inputsource} does not exist.\")\n","        # print(f\"Source table {inputsource} exists.\")\n","    # Log success\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=20, \n","        run_step='Success: Source object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail=None, \n","        statement_text=f\"Check table {source_table_path}\" \n","    )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=21, \n","        run_step='Failure: Source object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {source_table_path}\" \n","    )\n","    raise Exception(f\"Source table {inputsource} does not exist.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":75,"statement_ids":[75],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.0047042Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:16.2460757Z","execution_finish_time":"2025-03-18T20:31:18.535911Z","parent_msg_id":"205f0f42-ed99-45fb-8a81-4ee3f14e028f"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 75, Finished, Available, Finished)"},"metadata":{}}],"execution_count":73,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ca19b45-ff54-4b5f-bf07-efa45e17afad"},{"cell_type":"code","source":["#2 Check if the target table exist\n","try:\n","    if not mssparkutils.fs.exists(target_table_path):\n","        raise Exception(f\"Target table {outputtarget} does not exist.\")\n","      # print(f\"Target table {outputtarget} exists.\")\n"," \n","    # success\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=30, \n","        run_step='Success: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail=None, \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=31, \n","        run_step='Failure: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","    raise Exception(f\"Target table {outputtarget} does not exist.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":76,"statement_ids":[76],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.0813144Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:18.5388459Z","execution_finish_time":"2025-03-18T20:31:19.9666692Z","parent_msg_id":"0227c96c-1233-4a65-a76b-e513f2728f86"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 76, Finished, Available, Finished)"},"metadata":{}}],"execution_count":74,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d0ad13e-7e72-477d-8adb-2c09f7a5ab1c"},{"cell_type":"code","source":["#3 Column counts mismatch\n","try:\n","    source_df = spark.read.format(\"delta\").load(source_table_path)\n","    target_df = spark.read.format(\"delta\").load(target_table_path)\n","\n","    source_column_count = len(source_df.columns)\n","    target_column_count = len(target_df.columns)\n","\n","    # print(f\"Source table column count: {source_column_count}\")\n","    # print(f\"Target table column count: {target_column_count}\")\n","\n","    if not source_column_count == target_column_count:\n","        raise Exception(f\"Missing columns in source/target object\")\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=40, \n","        run_step='Success: Column counts match', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail= None, \n","        statement_text=f\"Check table {inputsource}\" \n","        )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=41, \n","        run_step='Failure: Missing column(s) in source/target object', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {inputsource}\" \n","    )\n","    raise Exception(f\"Column count mismatch for {inputsource}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":77,"statement_ids":[77],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.2662318Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:19.9695552Z","execution_finish_time":"2025-03-18T20:31:24.6105625Z","parent_msg_id":"935f3627-94cf-46f1-b2f8-ef8628d6f1d4"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 77, Finished, Available, Finished)"},"metadata":{}}],"execution_count":75,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"8681edec-6960-4524-af0c-d96335d41918"},{"cell_type":"code","source":["#4 Column name mismatch\n","try:\n","    source_columns = set(source_df.columns)\n","    target_columns = set(target_df.columns)\n","\n","    # check for column mismatch\n","    column_name_mismatches = source_columns.symmetric_difference(target_columns)\n","\n","    if column_name_mismatches:\n","        # mismatched column names list\n","        mismatch_details = \", \".join(column_name_mismatches)\n","        \n","        # log failure\n","        insert_log(\n","            lakehouse_name=lakehouse, \n","            schema_name=inputsourceschema, \n","            table_name=inputsource, \n","            job_run_timestamp=run_start, \n","            run_id=51, \n","            run_step='Failure: Column names mismatch', \n","            run_timestamp=datetime.now(), \n","            record_count=len(column_name_mismatches),\n","            step_fail=True,  \n","            statement_text=f\"Column name mismatches found {mismatch_details}\" \n","        )\n","        # Raise exception with the mismatch details\n","        raise Exception(f\"Column name mismatches found {mismatch_details}\")\n","    \n","    # Log success if no mismatches are found\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=50, \n","        run_step='Success: Column names match', \n","        run_timestamp=datetime.now(), \n","        record_count=None,\n","        step_fail=None, \n","        statement_text=f\"Check column names for {inputsource}\" \n","    )\n","except Exception as e:\n","    # catching unexpected errors\n","    raise Exception(f\"Error during column name check: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":78,"statement_ids":[78],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.4661621Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:24.6152191Z","execution_finish_time":"2025-03-18T20:31:26.8921871Z","parent_msg_id":"0b8292aa-7fff-4c60-a112-55ce06d31570"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 78, Finished, Available, Finished)"},"metadata":{}}],"execution_count":76,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a86b239a-ce2a-423a-9ffb-ab04bac00b80"},{"cell_type":"code","source":["#5 Column datatype mismatch\n","try:\n","    data_type_mismatches = []\n","\n","    for column in source_df.columns:\n","        if column in target_df.columns:\n","            # compare data types\n","            if str(source_df.schema[column].dataType) != str(target_df.schema[column].dataType):\n","                data_type_mismatches.append((column, str(source_df.schema[column].dataType), str(target_df.schema[column].dataType)))\n","\n","    if data_type_mismatches:\n","        # mismatched columns \n","        mismatch_details = \"\\n\".join([f\"Column: {col}, Source Data Type: {src_type}, Target Data Type: {tgt_type}\" \n","                                     for col, src_type, tgt_type in data_type_mismatches])\n","        \n","        # log failure with mismatch\n","        insert_log(\n","            lakehouse_name=lakehouse, \n","            schema_name=inputsourceschema, \n","            table_name=inputsource, \n","            job_run_timestamp=run_start, \n","            run_id=61, \n","            run_step='Failure: Column data type mismatch', \n","            run_timestamp=datetime.now(), \n","            record_count=len(data_type_mismatches),\n","            step_fail=True,  \n","            statement_text=f\"Data type mismatches found in the following columns:\\n{mismatch_details}\" \n","        )\n","        raise Exception(f\"Data type mismatches found in the following columns:\\n{mismatch_details}\")\n","    \n","    # log sucsess\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=60, \n","        run_step='Success: Column data types match', \n","        run_timestamp=datetime.now(), \n","        record_count=None,\n","        step_fail=None, \n","        statement_text=f\"Check column data types for {inputsource}\" \n","    )\n","except Exception as e:\n","    # raise unexpected errors\n","    raise Exception(f\"Error during data type check {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":79,"statement_ids":[79],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.5762159Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:26.8952514Z","execution_finish_time":"2025-03-18T20:31:28.3508925Z","parent_msg_id":"26694325-55cc-4ed3-8366-2b4127f2033f"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 79, Finished, Available, Finished)"},"metadata":{}}],"execution_count":77,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e825334-0421-4904-8887-9f11cb567c32"},{"cell_type":"code","source":["# Print source table schema\n","source_df = spark.read.format(\"delta\").load(source_table_path)\n","source_df.printSchema()\n","\n","# Print target table schema\n","target_df = spark.read.format(\"delta\").load(target_table_path)\n","target_df.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"a8238ed8-fe46-4f89-9ca1-f0bcf720e831"},{"cell_type":"markdown","source":["#### Data loads"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e73a4428-558f-491e-a75a-2c7424d8de7f"},{"cell_type":"markdown","source":["#### 1. Truncate and Reload"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6a4190f-deff-4dbf-8f80-7de8d7f5477c"},{"cell_type":"code","source":["# Truncate and Reload (TR)\n","if load_option == \"TR\":\n","    try:\n","        try:\n","            # Load delta table\n","            delta_table = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Delete all rows from the delta table\n","            delta_table.delete(\"1 = 1\") \n","            print(\"Truncate operation completed successfully\")             \n","\n","            # Log success for truncate\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=101, \n","                run_step=\"Success: Truncate target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=None,  # No record count for truncate\n","                step_fail=None,\n","                statement_text=None\n","            )  \n","\n","        except Exception as truncate_error:\n","            # Log failure for truncate\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=191,\n","                run_step=\"Execution Error: Truncate target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,  # No change for failure\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(truncate_error)}\"\n","            )\n","            print(f\"FAIL (190) Truncate failed. Error - {str(truncate_error)}\")\n","            raise truncate_error\n","\n","        try:\n","            # Load source data and append to target table\n","            input_df = spark.read.format(\"delta\").load(source_table_path)\n","            input_df.write.format(\"delta\").mode(\"append\").save(target_table_path)\n","\n","            # Log success for load\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=102,\n","                run_step=\"Success: Load target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=input_df.count(),  # Populate record count for success\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as load_error:\n","            # Log failure for load\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=192, \n","                run_step=\"Execution Error: Load target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,  # No change for failure\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(load_error)}\"\n","            )\n","            print(f\"FAIL (191) Load target failed. Error - {str(load_error)}\")\n","            raise load_error\n","\n","    except Exception as error:\n","        # Raise the error to be caught by the general failure block\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":80,"statement_ids":[80],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.6942466Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:28.3540395Z","execution_finish_time":"2025-03-18T20:31:28.6381717Z","parent_msg_id":"1b390eb8-6d47-448a-b74a-fee0c82745b9"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 80, Finished, Available, Finished)"},"metadata":{}}],"execution_count":78,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78c21c8f-baab-4112-b7a4-763a487f6b09"},{"cell_type":"markdown","source":["#### 2. LDUI (Logical Delete Update Insert)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a24d6919-35c3-43fe-a748-f034d75ff9ac"},{"cell_type":"code","source":["# Logical delete, update and insert\n","if load_option == \"LDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=291,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=201, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = spark.read.format(\"delta\").load(source_table_path)\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude audit columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).execute()\n","\n","            print(\"LDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=202, \n","                run_step=\"Success: LDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=292,\n","                run_step=\"Execution Error: LDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (292) LDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":81,"statement_ids":[81],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.8222403Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:28.6413651Z","execution_finish_time":"2025-03-18T20:31:28.9557416Z","parent_msg_id":"f665a33c-90e8-476e-9277-59318e3c69bf"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 81, Finished, Available, Finished)"},"metadata":{}}],"execution_count":79,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8f148551-a1ee-4685-ad54-419888b6faf7"},{"cell_type":"markdown","source":["#### 3. DLDUI (Derived Logical Delete, Update & Insert)\n","#### Used to refresh aggregated or synthesized objects where no action type is available from source"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"454b8573-155a-403c-bb14-3d7667d4f0e3"},{"cell_type":"code","source":["# Derived logical delete, update and insert\n","if load_option == \"DLDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=391,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=301, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = spark.read.format(\"delta\").load(source_table_path)\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude primary key columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).whenNotMatchedBySourceUpdate(\n","                condition=\"target.action_type != 'D'\",  # get records with action_type <> 'D'\n","                set={\n","                    \"action_type\": \"'D'\",  # Set action_type to 'D' for deleted records\n","                    \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","                }\n","            ).execute()\n","\n","            print(\"DLDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=302, \n","                run_step=\"Success: DLDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=392,\n","                run_step=\"Execution Error: DLDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (392) DLDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":82,"statement_ids":[82],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:08.9134031Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:28.9588133Z","execution_finish_time":"2025-03-18T20:31:29.2656567Z","parent_msg_id":"5ba8fcb7-8bbc-419a-a478-bc1a99e1b90a"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 82, Finished, Available, Finished)"},"metadata":{}}],"execution_count":80,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false},"editable":true,"run_control":{"frozen":false}},"id":"3e8176e2-b4fa-471c-9fec-ddc6f914318f"},{"cell_type":"markdown","source":["### 4. CLDUI (Controlled Logical Delete Update & Insert)\n","#### Performs targeted updates controlled by timestamp from last successful execution\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a886a9f-596d-4907-adc8-a51f619b1049"},{"cell_type":"code","source":["# Controlled Logical delete, update and insert\n","\n","if load_option == \"CLDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=491,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=401, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = spark.read.format(\"delta\").load(source_table_path)\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude audit key columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Fetch control_start_timestamp from materialization_control_table\n","            control_table_query = f\"\"\"\n","                SELECT control_start_timestamp\n","                FROM materialization_control\n","                WHERE etl_database = '{inputsourceschema}'\n","                AND etl_view = '{inputsource}'\n","                and load_option ='{load_option}'\n","                \"\"\"\n","\n","            control_df = spark.sql(control_table_query)  \n","            control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","            # Check if control_df has rows\n","            if control_df.count() > 0:\n","                # Get the first row\n","                row = control_df.collect()[0]\n","                # Check if the value is not None before using it\n","                if row[\"control_start_timestamp\"] is not None:\n","                    control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","            # Pull incremental data from the source table\n","            source_df = spark.read.format(\"delta\").load(source_table_path) \\\n","            .filter(f\"row_update_timestamp >= '{control_start_timestamp}'\")\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).execute()\n","\n","            print(\"CLDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=402, \n","                run_step=\"Success: CLDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=492,\n","                run_step=\"Execution Error: CLDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (492) CLDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":83,"statement_ids":[83],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:09.0065607Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:29.2685294Z","execution_finish_time":"2025-03-18T20:31:39.0964989Z","parent_msg_id":"22c6bb49-7bb6-4e20-836a-2a93962528bd"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 83, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CLDUI load completed successfully\n"]}],"execution_count":81,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3837f91f-1cce-4f56-80f9-d8b2a9ea5241"},{"cell_type":"code","source":["control_table_query = f\"\"\"\n","    SELECT control_start_timestamp\n","    FROM materialization_control\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    and load_option ='{load_option}'\n","    \"\"\"\n","control_df = spark.sql(control_table_query)  \n","control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","# Check if control_df has rows\n","if control_df.count() > 0:\n","    # Get the first row\n","    row = control_df.collect()[0]\n","    # Check if the value is not None before using it\n","    if row[\"control_start_timestamp\"] is not None:\n","        control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","\n","# Pull incremental data from the source table\n","source_df = spark.read.format(\"delta\").load(source_table_path) \\\n","   .filter(f\"row_update_timestamp >= '{control_start_timestamp}'\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"9cb67737-71d3-476c-bd18-2dfbca4919c0"},{"cell_type":"code","source":["# Define the query\n","control_table_query = f\"\"\"\n","    SELECT control_start_timestamp\n","    FROM materialization_control\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND load_option = '{load_option}'\n","\"\"\"\n","\n","# Execute the query\n","control_df = spark.sql(control_table_query)\n","control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","\n","# Check if control_df has rows\n","if control_df.count() > 0:\n","    # Get the first row\n","    row = control_df.collect()[0]\n","    # Check if the value is not None before using it\n","    if row[\"control_start_timestamp\"] is not None:\n","        control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","# Print the control_start_timestamp\n","print(\"Control Start Timestamp:\", control_start_timestamp)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":45,"statement_ids":[45],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:21:31.2560174Z","session_start_time":null,"execution_start_time":"2025-03-18T20:21:31.257656Z","execution_finish_time":"2025-03-18T20:21:32.1435833Z","parent_msg_id":"440e3de4-29f7-4141-8f65-159e3a793375"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 45, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Control Start Timestamp: 1900-01-01 00:00:00\n"]}],"execution_count":43,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"aab20bcd-0e85-43eb-89e1-9dcddec5aeab"},{"cell_type":"code","source":["# Check if source_df has any records\n","if source_df.count() > 0:\n","    # Get max timestamp from source table\n","    max_row_update_timestamp = source_df.selectExpr(\"max(row_update_timestamp)\").collect()[0][0]\n","\n","    # Update the control_start_timestamp in materialization_control table\n","    control_update_query = f\"\"\"\n","        MERGE INTO materialization_control AS target\n","        USING (\n","            SELECT '{inputsourceschema}' AS etl_database,\n","                   '{inputsource}' AS etl_view,\n","                   '{load_option}' AS load_option,\n","                   to_timestamp('{max_row_update_timestamp}') AS control_start_timestamp\n","        ) AS source\n","        ON target.etl_database = source.etl_database\n","           AND target.etl_view = source.etl_view\n","           AND target.load_option = source.load_option\n","        WHEN MATCHED THEN\n","            UPDATE SET target.control_start_timestamp = source.control_start_timestamp\n","        WHEN NOT MATCHED THEN\n","            INSERT (etl_database, etl_view, load_option, control_start_timestamp)\n","            VALUES (source.etl_database, source.etl_view, source.load_option, source.control_start_timestamp)\n","    \"\"\"\n","    print(spark.sql(control_update_query))\n","else:\n","    print(\"No records to process. Skipping update of control_start_timestamp.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":84,"statement_ids":[84],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:09.1091998Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:39.0994255Z","execution_finish_time":"2025-03-18T20:31:42.443414Z","parent_msg_id":"a0678bb2-4897-4de4-8836-d74b3540c3e9"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 84, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]\n"]}],"execution_count":82,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d70ef25b-7296-4dec-94cd-54f80fa38762"},{"cell_type":"markdown","source":["#### Exception for unsupported load option"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0005afd3-bf6d-40e4-bded-9d4d2564fb3c"},{"cell_type":"code","source":["# Unsupported Load Option\n","if load_option not in [\"TR\",\"LDUI\",\"DLDUI\",\"CLDUI\"]:\n","    try:\n","        raise ValueError(f\" Unsupported load_option - {load_option}\")\n","    except Exception as e:\n","        # Log failure for unsupported load_option\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=999,  \n","            run_step=\"Execution Error: Unsupported load_option\",\n","            run_timestamp=datetime.now(),\n","            record_count=1,  # No change for failure\n","            step_fail=\"True\",\n","            statement_text=f\"Error - {str(e)}\"\n","        )\n","        print(f\"FAIL (999) Unsupported load_option. Error - {str(e)}\")\n","        raise e"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":85,"statement_ids":[85],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:09.1648458Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:42.4459806Z","execution_finish_time":"2025-03-18T20:31:42.7367824Z","parent_msg_id":"5d14d5f2-c7a1-46bc-acb9-27123a981f3a"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 85, Finished, Available, Finished)"},"metadata":{}}],"execution_count":83,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5bbfd40d-f00d-40e0-8330-839e4e505c5b"},{"cell_type":"markdown","source":["### Load complete"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26ab9a4d-66eb-4c55-b2b5-3526a265f255"},{"cell_type":"code","source":["# Load complete check\n","if load_option == \"TR\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=200, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","elif load_option == \"LDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=300, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    ) \n","elif load_option == \"DLDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=400, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","elif load_option == \"CLDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=500, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","    # Update materialization_control table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_control\n","        SET \n","            extract_start_timestamp = '{run_start}',\n","            extract_end_timestamp = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    \"\"\")\n","\n","    # update materialization_load table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_load\n","        SET\n","            last_successful_execution = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND lakehouse_name = '{lakehouse}'\n","        AND input_source_schema = '{inputsourceschema}'\n","        AND input_source = '{inputsource}'\n","        AND output_target_schema = '{outputtargetschema}'\n","        AND output_target = '{outputtarget}'\n","        AND load_option = '{load_option}'\n","    \"\"\")\n","\n","    print(f\"Load completed successfully for load_option: {load_option}.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":86,"statement_ids":[86],"state":"finished","livy_statement_state":"available","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","normalized_state":"finished","queued_time":"2025-03-18T20:31:09.2461531Z","session_start_time":null,"execution_start_time":"2025-03-18T20:31:42.7398939Z","execution_finish_time":"2025-03-18T20:31:48.8119498Z","parent_msg_id":"906e5b94-d26f-4594-a649-f42f60504eae"},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 86, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Load completed successfully for load_option: CLDUI.\n"]}],"execution_count":84,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f74b498f-29bf-4de0-a347-0777ee9f4e7a"},{"cell_type":"markdown","source":["#### Validate results"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61818c32-4d0f-4bb7-a86e-52535f6bc2d8"},{"cell_type":"code","source":["# Verify the results in materialization_log\n","display(spark.sql(f\"\"\"\n","SELECT * FROM materialization_log\n","WHERE schema_name = '{inputsourceschema}' AND table_name = '{inputsource}'\n","\"\"\"))\n","\n","# Verify the results in materialization_control\n","display(spark.sql(f\"\"\"\n","SELECT * FROM materialization_control\n","WHERE etl_database = '{inputsourceschema}' AND etl_view = '{inputsource}'\n","\"\"\"))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"7b0f5410-b8fb-4aaf-92ea-1cfe6ebe16e8"},{"cell_type":"code","source":["%%sql\n","select * from materialization_load where 1=1 and lakehouse_name ='silver_sapecc_lakehouse'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"7d0abca0-cc52-4a6c-bf68-e7fa4f65bc03"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d0ace1d4-3de2-4379-9bea-a092648d9ae3","default_lakehouse_name":"utilities_lakehouse","default_lakehouse_workspace_id":"45996e9c-972f-49c2-89b8-4f3f04269611","known_lakehouses":[{"id":"d0ace1d4-3de2-4379-9bea-a092648d9ae3"},{"id":"b70a9a68-ce81-4761-86b7-5005c5e65fed"},{"id":"f0b6b768-1033-4592-962a-c84a93795bd4"}]}}},"nbformat":4,"nbformat_minor":5}