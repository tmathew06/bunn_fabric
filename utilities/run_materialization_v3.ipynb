{"cells":[{"cell_type":"code","source":["# import libraries\n","\n","from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import col, lit, count, when, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n","from datetime import datetime\n","from delta.tables import DeltaTable\n","import os\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.064845Z","session_start_time":"2025-05-12T14:44:02.0658636Z","execution_start_time":"2025-05-12T14:44:12.4115828Z","execution_finish_time":"2025-05-12T14:44:12.8423936Z","parent_msg_id":"dd34e740-d41d-4acc-826a-1d5e4a16ca0e"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8181829a-5853-43f4-bb3a-56c7f49cd74f"},{"cell_type":"code","source":["# initialize session\n","spark = SparkSession.builder \\\n","    .appName(\"run_materialization\") \\\n","    .getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0683011Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:12.8441344Z","execution_finish_time":"2025-05-12T14:44:13.112509Z","parent_msg_id":"d0ec5c89-c9ee-4e37-9a0a-cc05d6736c41"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89bd6d73-d177-488e-9349-93efe827c96c"},{"cell_type":"markdown","source":["#### Input parameters"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"bd080a02-7e6a-4818-a1fc-6a1534fce8cd"},{"cell_type":"code","source":["workspace = 'BUNN_Foundation_NONPROD'\n","lakehouse = 'silver_sapecc_lakehouse'\n","inputsourceschema = 'materialized_etl'\n","inputsource = 'orders_etl'\n","outputtargetschema = 'materialized_t'\n","outputtarget = 'orders'\n","update_control_table = 0\n","load_option = 'TR' \n","run_start = datetime.now()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0700494Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:13.114398Z","execution_finish_time":"2025-05-12T14:44:13.3604538Z","parent_msg_id":"a0647887-cd1d-43fd-9c15-5622aa9d254f"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":false},"tags":["parameters"]},"id":"925f5d41-5227-4b5c-8fb0-e71ab2d767d7"},{"cell_type":"code","source":["# Begin materialization process\n","\n","spark.sql(f\"\"\"\n","INSERT INTO utilities_lakehouse.materialization_log (schema_name, table_name, job_run_timestamp, run_id, run_step, run_timestamp, record_count, step_fail,statement_text, lakehouse_name)\n","VALUES ('{inputsourceschema}', '{inputsource}', '{run_start}', 10, 'begin materialization', current_timestamp(),null,null, null,'{lakehouse}')\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0719615Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:13.362238Z","execution_finish_time":"2025-05-12T14:44:24.0709763Z","parent_msg_id":"e752e9f0-5f52-4a68-b97c-372892c6c995"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"DataFrame[]"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9e4174a-5a1f-4419-967d-f3293468c59f"},{"cell_type":"code","source":["# insert/update materialization control table\n","\n","if update_control_table == 1:\n","    # update materialization_control if update_control_table = 1\n","    query = f\"\"\"\n","    UPDATE materialization_control\n","    SET control_start_timestamp = current_timestamp()\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND lakehouse_name = '{lakehouse}'\n","    AND load_option = '{load_option}'\n","    \"\"\"\n","    spark.sql(query)    \n","else:\n","    # insert into materialization_control if no matching record exists\n","    query = f\"\"\"\n","    INSERT INTO materialization_control (etl_database, etl_view, status_code, load_option, extract_start_timestamp, \n","                            extract_end_timestamp, script_header, last_script_generate_timestamp, is_active, control_start_timestamp, lakehouse_name)\n","    SELECT '{inputsourceschema}', '{inputsource}', 'active', '{load_option}', '{run_start}',null, null, null, null, null, '{lakehouse}'\n","    WHERE NOT EXISTS (\n","        SELECT 1\n","        FROM materialization_control\n","        WHERE etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    )\n","    \"\"\"\n","    spark.sql(query)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0738383Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:24.0730214Z","execution_finish_time":"2025-05-12T14:44:31.8747766Z","parent_msg_id":"db63a24d-1f6c-49dd-8a6c-62de9665a8ba"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cebc1052-0ad5-45ae-8e62-f3b72d92f122"},{"cell_type":"code","source":["# update materialization load table execution times\n","\n","spark.sql(f\"\"\"\n","update materialization_load\n","    set last_execution = current_timestamp\n","    WHERE 1=1\n","    and lakehouse_name = '{lakehouse}'\n","    and input_source_schema = '{inputsourceschema}'\n","    and input_source = '{inputsource}'\n","    and output_target_schema = '{outputtargetschema}'\n","    and output_target = '{outputtarget}'\n","    and load_option = '{load_option}'\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0759185Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:31.8770247Z","execution_finish_time":"2025-05-12T14:44:38.1323736Z","parent_msg_id":"48a985e6-299b-4ef1-9681-b5957c15e248"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"DataFrame[num_affected_rows: bigint]"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ad19741-54a0-42f3-9b6f-f6dcf83a5317"},{"cell_type":"markdown","source":["#### Metadata checks"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d6ffc79e-f16a-4057-83b4-10f8bc62c567"},{"cell_type":"code","source":["# define input/output table path\n","\n","#source_table_path  = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{inputsourceschema}/{inputsource}\"\n","target_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{outputtargetschema}/{outputtarget}\"\n","log_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/utilities_lakehouse.Lakehouse/Tables/materialization_log\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0777385Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:38.1346807Z","execution_finish_time":"2025-05-12T14:44:38.3903206Z","parent_msg_id":"24f4098e-ac89-496a-8e7d-0b56387de4d8"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10041612-bf85-4617-bc91-1bcc6f9cc607"},{"cell_type":"code","source":["# define log schema explicitly\n","\n","log_schema = StructType([\n","    StructField(\"lakehouse_name\", StringType(), True),\n","    StructField(\"schema_name\", StringType(), True),\n","    StructField(\"table_name\", StringType(), True),\n","    StructField(\"job_run_timestamp\", TimestampType(), True),\n","    StructField(\"run_id\", IntegerType(), True), \n","    StructField(\"run_step\", StringType(), True),\n","    StructField(\"run_timestamp\", TimestampType(), True),\n","    StructField(\"record_count\", IntegerType(), True),\n","    StructField(\"step_fail\", StringType(), True),\n","    StructField(\"statement_text\", StringType(), True)\n","])\n","\n","# insert log function\n","\n","def insert_log(lakehouse_name, schema_name, table_name, job_run_timestamp,\n","             run_id, run_step, run_timestamp, record_count, step_fail, statement_text):\n","    log_data = [\n","        Row(\n","            lakehouse_name=lakehouse_name,\n","            schema_name=schema_name,\n","            table_name=table_name,\n","            job_run_timestamp=job_run_timestamp,\n","            run_id=run_id,\n","            run_step=run_step,\n","            run_timestamp=run_timestamp,\n","            record_count = record_count,\n","            step_fail=step_fail,\n","            statement_text=statement_text\n","        )\n","    ]\n","    log_df = spark.createDataFrame(log_data, schema=log_schema)\n","    log_df.write.format(\"delta\").mode(\"append\").save(log_table_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0795569Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:38.392315Z","execution_finish_time":"2025-05-12T14:44:38.6430011Z","parent_msg_id":"59a33818-2e79-4a8d-aa33-86201abc19f0"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea736ca4-1f58-407e-b5f3-3ff6af8a9b84"},{"cell_type":"markdown","source":["##### Reading input data from a temporary view"],"metadata":{},"id":"20c37c9d"},{"cell_type":"code","source":["# Input Source (Temporary View from notebook)\n","is_source_temp_view = False \n","\n","# actual name to use with spark.table()\n","source_temp_view_name_for_spark = \"\"\n","\n","# current_source_logical_name \n","current_source_logical_name = f\"{inputsourceschema}.{inputsource}\"\n","\n","print(f\"Input Source Determination for '{current_source_logical_name}'\")\n","\n","notebook_to_run = inputsource \n","\n","temp_view_expected_name_for_spark = inputsource\n","print(f\"Expecting temp view from notebook. Attempting to run notebook: '{notebook_to_run}' to create temp view: '{temp_view_expected_name_for_spark}'\")\n","try:\n","    exit_value = mssparkutils.notebook.run(notebook_to_run, timeout_seconds=1200, arguments={\"useRootDefaultLakehouse\": True})\n","\n","    if spark.catalog.tableExists(temp_view_expected_name_for_spark):\n","        is_source_temp_view = True\n","        source_temp_view_name_for_spark = temp_view_expected_name_for_spark\n","        source_table_path = \"\" # Explicitly set to empty as it's not a physical path source\n","        print(f\"SUCCESS: Ran notebook '{notebook_to_run}'. Temporary view '{source_temp_view_name_for_spark}' is available.\")\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=15,\n","            run_step=f'Success: Executed notebook {notebook_to_run} for temp view',\n","            run_timestamp=datetime.now(),\n","            record_count=None,\n","            step_fail=None,\n","            statement_text=f\"Using temp view: {current_source_logical_name} (actual Spark name: {source_temp_view_name_for_spark})\"\n","        )\n","    else:\n","        error_msg = f\"Notebook '{notebook_to_run}' ran, but temp view '{temp_view_expected_name_for_spark}' was NOT created.\"\n","        print(f\"ERROR: {error_msg}\")\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=16,\n","            run_step=f'Failure: Temp view not created by {notebook_to_run}',\n","            run_timestamp=datetime.now(),\n","            record_count=1,\n","            step_fail=\"True\",\n","            statement_text=error_msg\n","        )\n","        raise Exception(error_msg)\n","except Exception as e_notebook_run:\n","    error_msg = f\"Failed to find/run notebook '{notebook_to_run}', or the notebook itself failed: {str(e_notebook_run)}\"\n","    print(f\"ERROR: {error_msg}\")\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=17,\n","        run_step=f'Failure: Executing source notebook {notebook_to_run}',\n","        run_timestamp=datetime.now(),\n","        record_count=1,\n","        step_fail=\"True\",\n","        statement_text=error_msg\n","    )\n","    raise Exception(error_msg)\n","\n","print(f\"Input Source Determination Complete. Using Logical Source: '{current_source_logical_name}'\")\n","# Modified the print output here:\n","if is_source_temp_view:\n","    print(f\"Actual Spark reference for temp view: '{source_temp_view_name_for_spark}'\")\n","else:\n","    # This case implies the notebook run failed to set is_source_temp_view,\n","      print(f\"WARNING: Temp view '{temp_view_expected_name_for_spark}' was expected but 'is_source_temp_view' is False. This indicates a problem in the notebook execution flow.\")\n","\n","# function to read the source dataframe consistently\n","def get_source_dataframe():\n","    if is_source_temp_view:\n","        print(f\"Reading from temporary view: {source_temp_view_name_for_spark}\")\n","        return spark.table(source_temp_view_name_for_spark)\n","    else:\n","        # If is_source_temp_view is False here, it means the notebook run for view creation failed \n","        error_msg_get_df = f\"Critical Error: Expected source to be temporary view '{source_temp_view_name_for_spark}', but 'is_source_temp_view' is False. View was not properly created or identified.\"\n","        print(f\"ERROR in get_source_dataframe: {error_msg_get_df}\")\n","        raise Exception(error_msg_get_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0816902Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:38.6450666Z","execution_finish_time":"2025-05-12T14:44:54.7400137Z","parent_msg_id":"11b4ba4c-4aae-457f-9673-eefebd812d16"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Input Source Determination for 'materialized_etl.orders_etl'\nExpecting temp view from notebook. Attempting to run notebook: 'orders_etl' to create temp view: 'orders_etl'\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrun-result+json":{"run_id":"2e659596-3ed3-4dab-a48e-1149c5ee1fbd","in_pipeline":false,"notebook_name":"orders_etl","snapshot_path":"","error":null,"session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","spark_pool":"Starter Pool","capacity_id":"84D3AB71-8A51-4556-A751-4B7445A8C19A","workspace_id":"45996e9c-972f-49c2-89b8-4f3f04269611","root_artifact_id":"408c92d5-d951-40e0-bae4-ebd2b3434aec","artifact_id":"12ba0dc1-1458-42e9-8937-f3371bcf6df7","snapshot_status":"success","snapshot_error":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SUCCESS: Ran notebook 'orders_etl'. Temporary view 'orders_etl' is available.\nInput Source Determination Complete. Using Logical Source: 'materialized_etl.orders_etl'\nActual Spark reference for temp view: 'orders_etl'\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"537e5711"},{"cell_type":"code","source":["#1 Check if the source temporary view can be accessed\n","\n","try:\n","    print(f\"Validating determined source (Temporary View): {current_source_logical_name}\")\n","    # The get_source_dataframe() call itself will fail if is_source_temp_view is false\n","    # or if the view (source_temp_view_name_for_spark) doesn't exist.\n","    source_df = get_source_dataframe() \n","    source_df.limit(0).collect() # Spark action to trigger read and validation of the view\n","\n","    # The is_source_temp_view check is now somewhat redundant here if get_source_dataframe() is robust,\n","    # but kept for explicit logging.\n","    if is_source_temp_view:\n","        validation_statement_text = f\"Temporary View '{current_source_logical_name}' successfully validated by read attempt. (Actual Spark view: {source_temp_view_name_for_spark})\"\n","    else:\n","        # This case should ideally not be reached if the previous cell's logic is sound.\n","        validation_statement_text = f\"ERROR: Expected a temporary view, but 'is_source_temp_view' is False. Source '{current_source_logical_name}' could not be validated as a view.\"\n","        # Raise an immediate error here as it's a logic contradiction\n","        raise Exception(\"Source validation logic error: Expected temporary view, but not identified as such.\")\n","\n","\n","    print(f\"SUCCESS: {validation_statement_text}\")\n","    \n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=20,\n","        run_step='Success: Source temporary view validation', # Modified step name\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=validation_statement_text\n","    )\n","except Exception as e:\n","    error_message_detail = f\"Validation failed for source temporary view '{current_source_logical_name}': {str(e)}\"\n","    print(f\"ERROR: {error_message_detail}\")\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=21,\n","        run_step='Failure: Source temporary view validation', # Modified step name\n","        run_timestamp=datetime.now(),\n","        record_count=1,\n","        step_fail=\"True\",\n","        statement_text=error_message_detail\n","    )\n","    raise Exception(f\"Source temporary view validation failed for '{current_source_logical_name}': {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0834864Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:54.7421081Z","execution_finish_time":"2025-05-12T14:44:56.1914574Z","parent_msg_id":"6736a61c-5e9f-477a-87c9-7e351b78925c"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validating determined source (Temporary View): materialized_etl.orders_etl\nReading from temporary view: orders_etl\nSUCCESS: Temporary View 'materialized_etl.orders_etl' successfully validated by read attempt. (Actual Spark view: orders_etl)\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ca19b45-ff54-4b5f-bf07-efa45e17afad"},{"cell_type":"code","source":["#2 Check if the target table exist\n","try:\n","    if not mssparkutils.fs.exists(target_table_path):\n","        raise Exception(f\"Target table {outputtarget} does not exist.\")\n","      # print(f\"Target table {outputtarget} exists.\")\n"," \n","    # success\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=30, \n","        run_step='Success: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail=None, \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=31, \n","        run_step='Failure: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","    raise Exception(f\"Target table {outputtarget} does not exist.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0852114Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:56.1935593Z","execution_finish_time":"2025-05-12T14:44:58.4858699Z","parent_msg_id":"a38dd11d-8a32-467f-865b-477b576b4a3c"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d0ad13e-7e72-477d-8adb-2c09f7a5ab1c"},{"cell_type":"code","source":["#3 source and traget columns check\n","\n","try:\n","    source_df = get_source_dataframe()\n","    target_df = spark.read.format(\"delta\").load(target_table_path)\n","\n","    source_column_count = len(source_df.columns)\n","    target_column_count = len(target_df.columns)\n","\n","    if not source_column_count == target_column_count:\n","        error_msg = f\"Column count mismatch. Source ({current_source_logical_name}): {source_column_count}, Target ({outputtargetschema}.{outputtarget}): {target_column_count}\"\n","        raise Exception(error_msg)\n","    \n","    success_msg = f\"SUCCESS: Column counts match for source '{current_source_logical_name}' and target '{outputtargetschema}.{outputtarget}'.\"\n","    print(success_msg)\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=40,\n","        run_step='Success: Column counts match',\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=success_msg\n","    )\n","except Exception as e:\n","    error_message_detail = f\"Column count check failed for source '{current_source_logical_name}': {str(e)}\"\n","    print(f\"ERROR: {error_message_detail}\")\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=41,\n","        run_step='Failure: Missing column(s) in source/target object',\n","        run_timestamp=datetime.now(),\n","        record_count=1,\n","        step_fail=\"True\",\n","        statement_text=error_message_detail\n","    )\n","    raise Exception(f\"Column count check failed: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0871608Z","session_start_time":null,"execution_start_time":"2025-05-12T14:44:58.4881035Z","execution_finish_time":"2025-05-12T14:45:00.9326296Z","parent_msg_id":"dfe3dedb-14a1-49bb-8987-2aea85c247f1"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Reading from temporary view: orders_etl\nSUCCESS: Column counts match for source 'materialized_etl.orders_etl' and target 'materialized_t.orders'.\n"]}],"execution_count":12,"metadata":{"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":false}},"id":"8681edec-6960-4524-af0c-d96335d41918"},{"cell_type":"code","source":["#4 Column name mismatch\n","try:\n","    source_columns = set(source_df.columns)\n","    target_columns = set(target_df.columns)\n","\n","    # check for column mismatch\n","    column_name_mismatches = source_columns.symmetric_difference(target_columns)\n","\n","    if column_name_mismatches:\n","        # mismatched column names list\n","        mismatch_details = \", \".join(column_name_mismatches)\n","        \n","        # log failure\n","        insert_log(\n","            lakehouse_name=lakehouse, \n","            schema_name=inputsourceschema, \n","            table_name=inputsource, \n","            job_run_timestamp=run_start, \n","            run_id=51, \n","            run_step='Failure: Column names mismatch', \n","            run_timestamp=datetime.now(), \n","            record_count=len(column_name_mismatches),\n","            step_fail=True,  \n","            statement_text=f\"Column name mismatches found {mismatch_details}\" \n","        )\n","        # Raise exception with the mismatch details\n","        raise Exception(f\"Column name mismatches found {mismatch_details}\")\n","    \n","    # Log success if no mismatches are found\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=50, \n","        run_step='Success: Column names match', \n","        run_timestamp=datetime.now(), \n","        record_count=None,\n","        step_fail=None, \n","        statement_text=f\"Check column names for {inputsource}\" \n","    )\n","except Exception as e:\n","    # catching unexpected errors\n","    raise Exception(f\"Error during column name check: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0891863Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:00.9350539Z","execution_finish_time":"2025-05-12T14:45:05.5887372Z","parent_msg_id":"e657be5f-537a-4b4a-b512-0c18cb717ca5"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a86b239a-ce2a-423a-9ffb-ab04bac00b80"},{"cell_type":"code","source":["#5 Column datatype mismatch\n","try:\n","    data_type_mismatches = []\n","\n","    for column in source_df.columns:\n","        if column in target_df.columns:\n","            # compare data types\n","            if str(source_df.schema[column].dataType) != str(target_df.schema[column].dataType):\n","                data_type_mismatches.append((column, str(source_df.schema[column].dataType), str(target_df.schema[column].dataType)))\n","\n","    if data_type_mismatches:\n","        # mismatched columns \n","        mismatch_details = \"\\n\".join([f\"Column: {col}, Source Data Type: {src_type}, Target Data Type: {tgt_type}\" \n","                                     for col, src_type, tgt_type in data_type_mismatches])\n","        \n","        # log failure with mismatch\n","        insert_log(\n","            lakehouse_name=lakehouse, \n","            schema_name=inputsourceschema, \n","            table_name=inputsource, \n","            job_run_timestamp=run_start, \n","            run_id=61, \n","            run_step='Failure: Column data type mismatch', \n","            run_timestamp=datetime.now(), \n","            record_count=len(data_type_mismatches),\n","            step_fail=True,  \n","            statement_text=f\"Data type mismatches found in the following columns:\\n{mismatch_details}\" \n","        )\n","        raise Exception(f\"Data type mismatches found in the following columns:\\n{mismatch_details}\")\n","    \n","    # log sucsess\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=60, \n","        run_step='Success: Column data types match', \n","        run_timestamp=datetime.now(), \n","        record_count=None,\n","        step_fail=None, \n","        statement_text=f\"Check column data types for {inputsource}\" \n","    )\n","except Exception as e:\n","    # raise unexpected errors\n","    raise Exception(f\"Error during data type check {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0912391Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:05.5909714Z","execution_finish_time":"2025-05-12T14:45:07.8890736Z","parent_msg_id":"107b433e-030a-4d0e-a0f7-64973827e102"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e825334-0421-4904-8887-9f11cb567c32"},{"cell_type":"markdown","source":["#### Data loads"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"e73a4428-558f-491e-a75a-2c7424d8de7f"},{"cell_type":"markdown","source":["#### 1. Truncate and Reload"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d6a4190f-deff-4dbf-8f80-7de8d7f5477c"},{"cell_type":"code","source":["# 1# Truncate and Reload (TR)\n","if load_option == \"TR\":\n","    # Outer try-except for the whole TR process\n","    try:\n","        print(f\"TR Load Option: Initiating for target {target_table_path}\")\n","        \n","        # Get the source dataframe first. If this fails, we don't attempt any writes.\n","        try:\n","            source_df = get_source_dataframe() # Getting from the function\n","            source_record_count = source_df.count() # Get count for logging\n","            print(f\"TR Load: Source data retrieved with {source_record_count} records.\")\n","        except Exception as source_error:\n","            # Log failure related to getting source data\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema, \n","                table_name=inputsource,     \n","                job_run_timestamp=run_start,\n","                run_id=190,\n","                run_step=\"Execution Error: Retrieving source data for TR\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error retrieving source for TR - {str(source_error)}\"\n","            )\n","            print(f\"FAIL (190) TR: Failed to retrieve source data. Error - {str(source_error)}\")\n","            raise source_error # Re-raise to be caught by the outer general_error handler\n","\n","        try:\n","            # The overwrite operation effectively.\n","            print(f\"TR Load: Attempting to overwrite {target_table_path} with new data...\")\n","            source_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(target_table_path)\n","            print(f\"TR Load: Overwrite operation completed successfully for {target_table_path}.\")\n","\n","            # Log success for clearing of old data by overwrite\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=outputtargetschema, \n","                table_name=outputtarget,\n","                job_run_timestamp=run_start,\n","                run_id=101, \n","                run_step=\"Success: Target object cleared (by overwrite)\", \n","                run_timestamp=datetime.now(),\n","                record_count=None,\n","                step_fail=None,\n","                statement_text=f\"Target {target_table_path} cleared as part of overwrite.\"\n","            )\n","\n","            # Log success for to  writing of new data by overwrite\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=outputtargetschema, \n","                table_name=outputtarget,\n","                job_run_timestamp=run_start,\n","                run_id=102,\n","                run_step=\"Success: New data loaded to target object (by overwrite)\", \n","                run_timestamp=datetime.now(),\n","                record_count=source_record_count, \n","                step_fail=None,\n","                statement_text=f\"New data loaded to {target_table_path} as part of overwrite.\"\n","            )\n","\n","        except Exception as overwrite_error:\n","            error_message_detail = f\"Overwrite operation failed for {target_table_path}. Error - {str(overwrite_error)}\"\n","            print(f\"FAIL (191) TR: Overwrite failed. {error_message_detail}\") \n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=outputtargetschema,\n","                table_name=outputtarget,\n","                job_run_timestamp=run_start,\n","                run_id=191, \n","                run_step=\"Execution Error: Overwrite target object\", \n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=error_message_detail\n","            )\n","            raise overwrite_error # Re-raise to be caught by general_error handler\n","    except Exception as error:\n","        # Raise the error to be caught by the general failure block\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0930194Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:07.8913732Z","execution_finish_time":"2025-05-12T14:45:15.8325867Z","parent_msg_id":"5869a226-9c67-4045-b77f-274e40267448"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["TR Load Option: Initiating for target abfss://BUNN_Foundation_NONPROD@onelake.dfs.fabric.microsoft.com/silver_sapecc_lakehouse.Lakehouse/Tables/materialized_t/orders\nReading from temporary view: orders_etl\nTR Load: Source data retrieved with 3934 records.\nTR Load: Attempting to overwrite abfss://BUNN_Foundation_NONPROD@onelake.dfs.fabric.microsoft.com/silver_sapecc_lakehouse.Lakehouse/Tables/materialized_t/orders with new data...\nTR Load: Overwrite operation completed successfully for abfss://BUNN_Foundation_NONPROD@onelake.dfs.fabric.microsoft.com/silver_sapecc_lakehouse.Lakehouse/Tables/materialized_t/orders.\n"]}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78c21c8f-baab-4112-b7a4-763a487f6b09"},{"cell_type":"markdown","source":["#### 2. LDUI (Logical Delete Update Insert)"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a24d6919-35c3-43fe-a748-f034d75ff9ac"},{"cell_type":"code","source":["# Logical delete, update and insert\n","if load_option == \"LDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=291,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=201, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = get_source_dataframe()\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude audit columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).execute()\n","\n","            print(\"LDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=202, \n","                run_step=\"Success: LDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=292,\n","                run_step=\"Execution Error: LDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (292) LDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0950128Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:15.8350682Z","execution_finish_time":"2025-05-12T14:45:16.0888074Z","parent_msg_id":"f75d2813-9281-4040-b38d-501a9c355475"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8f148551-a1ee-4685-ad54-419888b6faf7"},{"cell_type":"markdown","source":["#### 3. DLDUI (Derived Logical Delete, Update & Insert)\n","#### Used to refresh aggregated or synthesized objects where no action type is available from source"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"454b8573-155a-403c-bb14-3d7667d4f0e3"},{"cell_type":"code","source":["# Derived logical delete, update and insert\n","if load_option == \"DLDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=391,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=301, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = get_source_dataframe()\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude primary key columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).whenNotMatchedBySourceUpdate(\n","                condition=\"target.action_type != 'D'\",  # get records with action_type <> 'D'\n","                set={\n","                    \"action_type\": \"'D'\",  # Set action_type to 'D' for deleted records\n","                    \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","                }\n","            ).execute()\n","\n","            print(\"DLDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=302, \n","                run_step=\"Success: DLDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=392,\n","                run_step=\"Execution Error: DLDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (392) DLDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0970284Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:16.0909497Z","execution_finish_time":"2025-05-12T14:45:16.3386821Z","parent_msg_id":"9e516255-7a3d-4b7e-9ffb-b3f52bb1acbc"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":false}},"id":"3e8176e2-b4fa-471c-9fec-ddc6f914318f"},{"cell_type":"markdown","source":["### 4. CLDUI (Controlled Logical Delete Update & Insert)\n","#### Performs targeted updates controlled by timestamp from last successful execution\n","\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"1a886a9f-596d-4907-adc8-a51f619b1049"},{"cell_type":"code","source":["# Controlled Logical delete, update and insert\n","\n","if load_option == \"CLDUI\":\n","    try:\n","        try:\n","            # Query metadata table\n","            meta_query = f\"\"\"\n","            SELECT source_key, target_key\n","            FROM bunn_meta\n","            WHERE source_schema = '{inputsourceschema}'\n","            AND source_table = '{inputsource}'\n","            AND target_schema = '{outputtargetschema}'\n","            AND target_table = '{outputtarget}'\n","            \"\"\"\n","            meta_df = spark.sql(meta_query)\n","\n","            # Check PK's exist for target\n","            if meta_df.count() == 0:\n","                # Log failure for primary key check\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=491,\n","                    run_step=\"Execution Error: Primary key metadata not found\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=1,\n","                    step_fail=\"True\",\n","                    statement_text=f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\"\n","                )\n","                raise ValueError(f\"No primary key metadata found for target_schema={inputsourceschema}, and target_table={inputsource} in bunn_meta table.\")\n","\n","            # Log success for primary key check\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=401, \n","                run_step=\"Success: Primary key metadata found\",\n","                run_timestamp=datetime.now(),\n","                record_count=meta_df.count(),\n","                step_fail=None,\n","                statement_text=f\"Primary keys: {meta_df.collect()}\"\n","            )\n","\n","           # Collect source and target key columns\n","            primary_key_metadata = meta_df.collect()\n","            source_keys = [row[\"source_key\"] for row in primary_key_metadata]\n","            target_keys = [row[\"target_key\"] for row in primary_key_metadata]\n","\n","            # Construct the merge condition dynamically\n","            merge_conditions = [\n","                f\"target.{target_key} = source.{source_key}\"\n","                for source_key, target_key in zip(source_keys, target_keys)\n","            ]\n","            merge_condition = \" AND \".join(merge_conditions)\n","\n","            # Load source and target tables\n","            source_df = get_source_dataframe()\n","            target_df = DeltaTable.forPath(spark, target_table_path)\n","\n","            # Get a list of columns from source and target\n","            source_columns = source_df.columns\n","            target_columns = target_df.toDF().columns\n","\n","            # Exclude audit columns\n","            non_key_columns = [\n","                col for col in source_columns\n","                if col not in source_keys  # Exclude audit key columns\n","                and col not in [\"action_type\", \"row_insert_timestamp\", \"row_update_timestamp\"] \n","            ]\n","\n","            # Construct the update condition to check if any non-key column has changed\n","            update_conditions = [\n","                f\"source.{col} <> target.{col}\" \n","                for col in non_key_columns\n","            ]\n","            update_condition = \" OR \".join(update_conditions)\n","\n","            set_clause = {\n","                col: f\"source.{col}\" for col in non_key_columns  \n","            }\n","            set_clause.update({\n","                \"action_type\": \"'U'\",  # Set action_type to 'U' for updates\n","                \"row_update_timestamp\": \"current_timestamp()\"  # Update row_update_timestamp\n","            })\n","\n","            # Fetch control_start_timestamp from materialization_control_table\n","            control_table_query = f\"\"\"\n","                SELECT control_start_timestamp\n","                FROM materialization_control\n","                WHERE etl_database = '{inputsourceschema}'\n","                AND etl_view = '{inputsource}'\n","                and load_option ='{load_option}'\n","                \"\"\"\n","\n","            control_df = spark.sql(control_table_query)  \n","            control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","            # Check if control_df has rows\n","            if control_df.count() > 0:\n","                # Get the first row\n","                row = control_df.collect()[0]\n","                # Check if the value is not None before using it\n","                if row[\"control_start_timestamp\"] is not None:\n","                    control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","            # Pull incremental data from the source table\n","            source_df = get_source_dataframe() \\\n","            .filter(f\"row_update_timestamp >= '{control_start_timestamp}'\")\n","\n","            # Perform merge operation\n","            target_df.alias(\"target\").merge(\n","                source_df.alias(\"source\"),\n","                merge_condition\n","            ).whenMatchedUpdate(\n","                condition=update_condition, \n","                set=set_clause  \n","            ).whenNotMatchedInsertAll(\n","            ).execute()\n","\n","            print(\"CLDUI load completed successfully\")\n","\n","            # Log for success\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=402, \n","                run_step=\"Success: CLDUI load completed\",\n","                run_timestamp=datetime.now(),\n","                record_count=source_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as merge_error:\n","            # Log for failure\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=492,\n","                run_step=\"Execution Error: CLDUI load failed\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(merge_error)}\"\n","            )\n","            print(f\"FAIL (492) CLDUI load failed. Error - {str(merge_error)}\")\n","            raise merge_error\n","\n","    except Exception as error:\n","        # Raise the error for general failure\n","        raise error"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:02.0988352Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:16.3410056Z","execution_finish_time":"2025-05-12T14:45:16.5826272Z","parent_msg_id":"38ef750a-5a1b-4e51-b851-c0f6d85a844b"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3837f91f-1cce-4f56-80f9-d8b2a9ea5241"},{"cell_type":"code","source":["control_table_query = f\"\"\"\n","    SELECT control_start_timestamp\n","    FROM materialization_control\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    and load_option ='{load_option}'\n","    \"\"\"\n","control_df = spark.sql(control_table_query)  \n","control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","# Check if control_df has rows\n","if control_df.count() > 0:\n","    # Get the first row\n","    row = control_df.collect()[0]\n","    # Check if the value is not None before using it\n","    if row[\"control_start_timestamp\"] is not None:\n","        control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","\n","# Pull incremental data from the source table\n","source_df = spark.read.format(\"delta\").load(source_table_path) \\\n","   .filter(f\"row_update_timestamp >= '{control_start_timestamp}'\")\n"],"outputs":[],"execution_count":null,"metadata":{"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":true}},"id":"9cb67737-71d3-476c-bd18-2dfbca4919c0"},{"cell_type":"code","source":["# Define the query\n","control_table_query = f\"\"\"\n","    SELECT control_start_timestamp\n","    FROM materialization_control\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND load_option = '{load_option}'\n","\"\"\"\n","\n","# Execute the query\n","control_df = spark.sql(control_table_query)\n","control_start_timestamp = \"1900-01-01 00:00:00\"\n","\n","\n","# Check if control_df has rows\n","if control_df.count() > 0:\n","    # Get the first row\n","    row = control_df.collect()[0]\n","    # Check if the value is not None before using it\n","    if row[\"control_start_timestamp\"] is not None:\n","        control_start_timestamp = row[\"control_start_timestamp\"]\n","\n","# Print the control_start_timestamp\n","print(\"Control Start Timestamp:\", control_start_timestamp)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2025-03-18T20:21:32.1435833Z","execution_start_time":"2025-03-18T20:21:31.257656Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"440e3de4-29f7-4141-8f65-159e3a793375","queued_time":"2025-03-18T20:21:31.2560174Z","session_id":"6f1f94b2-98cf-4d7d-9b79-b816427ec58c","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":45,"statement_ids":[45]},"text/plain":"StatementMeta(, 6f1f94b2-98cf-4d7d-9b79-b816427ec58c, 45, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Control Start Timestamp: 1900-01-01 00:00:00\n"]}],"execution_count":43,"metadata":{"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":true}},"id":"aab20bcd-0e85-43eb-89e1-9dcddec5aeab"},{"cell_type":"code","source":["# Check if source_df has any records\n","if source_df.count() > 0:\n","    # Get max timestamp from source table\n","    max_row_update_timestamp = source_df.selectExpr(\"max(row_update_timestamp)\").collect()[0][0]\n","\n","    # Update the control_start_timestamp in materialization_control table\n","    control_update_query = f\"\"\"\n","        MERGE INTO materialization_control AS target\n","        USING (\n","            SELECT '{inputsourceschema}' AS etl_database,\n","                   '{inputsource}' AS etl_view,\n","                   '{load_option}' AS load_option,\n","                   to_timestamp('{max_row_update_timestamp}') AS control_start_timestamp\n","        ) AS source\n","        ON target.etl_database = source.etl_database\n","           AND target.etl_view = source.etl_view\n","           AND target.load_option = source.load_option\n","        WHEN MATCHED THEN\n","            UPDATE SET target.control_start_timestamp = source.control_start_timestamp\n","        WHEN NOT MATCHED THEN\n","            INSERT (etl_database, etl_view, load_option, control_start_timestamp)\n","            VALUES (source.etl_database, source.etl_view, source.load_option, source.control_start_timestamp)\n","    \"\"\"\n","    print(spark.sql(control_update_query))\n","else:\n","    print(\"No records to process. Skipping update of control_start_timestamp.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:03.329105Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:16.5845639Z","execution_finish_time":"2025-05-12T14:45:24.5282807Z","parent_msg_id":"2dc947ca-9ac4-4b2b-b23b-ba506b5b6043"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]\n"]}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d70ef25b-7296-4dec-94cd-54f80fa38762"},{"cell_type":"markdown","source":["#### Exception for unsupported load option"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"0005afd3-bf6d-40e4-bded-9d4d2564fb3c"},{"cell_type":"code","source":["# Unsupported Load Option\n","if load_option not in [\"TR\",\"LDUI\",\"DLDUI\",\"CLDUI\"]:\n","    try:\n","        raise ValueError(f\" Unsupported load_option - {load_option}\")\n","    except Exception as e:\n","        # Log failure for unsupported load_option\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=999,  \n","            run_step=\"Execution Error: Unsupported load_option\",\n","            run_timestamp=datetime.now(),\n","            record_count=1,  # No change for failure\n","            step_fail=\"True\",\n","            statement_text=f\"Error - {str(e)}\"\n","        )\n","        print(f\"FAIL (999) Unsupported load_option. Error - {str(e)}\")\n","        raise e"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:03.7676668Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:24.5304951Z","execution_finish_time":"2025-05-12T14:45:24.7983309Z","parent_msg_id":"94e5344e-b4d5-4a18-ad0d-fb77d9297ecf"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5bbfd40d-f00d-40e0-8330-839e4e505c5b"},{"cell_type":"markdown","source":["### Load complete"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"26ab9a4d-66eb-4c55-b2b5-3526a265f255"},{"cell_type":"code","source":["# Load complete check\n","if load_option == \"TR\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=200, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","elif load_option == \"LDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=300, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    ) \n","elif load_option == \"DLDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=400, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","elif load_option == \"CLDUI\":\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=500, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","    # Update materialization_control table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_control\n","        SET \n","            extract_start_timestamp = '{run_start}',\n","            extract_end_timestamp = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    \"\"\")\n","\n","    # update materialization_load table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_load\n","        SET\n","            last_successful_execution = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND lakehouse_name = '{lakehouse}'\n","        AND input_source_schema = '{inputsourceschema}'\n","        AND input_source = '{inputsource}'\n","        AND output_target_schema = '{outputtargetschema}'\n","        AND output_target = '{outputtarget}'\n","        AND load_option = '{load_option}'\n","    \"\"\")\n","\n","    print(f\"Load completed successfully for load_option: {load_option}.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"055e122a-0d87-4fa1-ad4a-f8f8239263d4","normalized_state":"finished","queued_time":"2025-05-12T14:44:03.9464126Z","session_start_time":null,"execution_start_time":"2025-05-12T14:45:24.8006339Z","execution_finish_time":"2025-05-12T14:45:26.1969261Z","parent_msg_id":"c66a6bb9-1b28-4675-8157-c27a1dbd6b0f"},"text/plain":"StatementMeta(, 055e122a-0d87-4fa1-ad4a-f8f8239263d4, 23, Finished, Available, Finished)"},"metadata":{}}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f74b498f-29bf-4de0-a347-0777ee9f4e7a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d0ace1d4-3de2-4379-9bea-a092648d9ae3","default_lakehouse_name":"utilities_lakehouse","default_lakehouse_workspace_id":"45996e9c-972f-49c2-89b8-4f3f04269611","known_lakehouses":[{"id":"d0ace1d4-3de2-4379-9bea-a092648d9ae3"},{"id":"b70a9a68-ce81-4761-86b7-5005c5e65fed"},{"id":"f0b6b768-1033-4592-962a-c84a93795bd4"}]}}},"nbformat":4,"nbformat_minor":5}