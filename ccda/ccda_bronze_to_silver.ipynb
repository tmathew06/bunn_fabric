{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, expr, current_timestamp, explode,\n",
        "    coalesce, array, when, max as spark_max, count as spark_count\n",
        ")\n",
        "from pyspark.sql.types import StringType, TimestampType\n",
        "from datetime import datetime\n",
        "from delta.tables import DeltaTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Input Parameters\n",
        "# Update these values to match your Fabric environment.\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Fabric workspace name\n",
        "workspace = 'BUNN_Foundation_NONPROD'\n",
        "\n",
        "# Lakehouse names\n",
        "bronze_lakehouse = 'bronze_clinical_lakehouse'\n",
        "silver_lakehouse = 'silver_clinical_lakehouse'\n",
        "utilities_lakehouse = 'utilities_lakehouse'\n",
        "\n",
        "# Bronze source table containing the raw JSON VARIANT column\n",
        "bronze_table = 'clinical_raw'\n",
        "\n",
        "# The column in the bronze table that holds the JSON VARIANT data\n",
        "variant_column = 'raw_data'\n",
        "\n",
        "# The column in the bronze table used for watermarking\n",
        "watermark_column = 'insert_timestamp'\n",
        "\n",
        "# Bronze table ID column for lineage tracking\n",
        "bronze_id_column = 'bronze_id'\n",
        "\n",
        "# List of target silver tables to process in this run\n",
        "target_tables = ['OBSERVATION', 'ENCOUNTER']\n",
        "\n",
        "# Capture the run start time for logging\n",
        "run_start = datetime.now()\n",
        "print(f\"Processing started at {run_start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Utility: Convert dot-notation JSON path to variant colon notation\n",
        "# -------------------------------------------------------------------\n",
        "# The mapping table stores paths as 'a.b.c' (dot notation) for readability.\n",
        "# Databricks VARIANT columns use colon notation: column:a:b:c\n",
        "# This function performs that conversion at runtime.\n",
        "\n",
        "def dot_to_variant_path(dot_path):\n",
        "    \"\"\"\n",
        "    Convert a dot-notation JSON path to variant colon-separated notation.\n",
        "\n",
        "    Example:\n",
        "        'organizer.component.observation.code._code'\n",
        "        becomes\n",
        "        'organizer:component:observation:code:_code'\n",
        "    \"\"\"\n",
        "    return dot_path.replace(\".\", \":\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Utility: Watermark management functions\n",
        "# -------------------------------------------------------------------\n",
        "# These functions read and update the high watermark in the\n",
        "# clinical_processing_log table to prevent reprocessing of data.\n",
        "\n",
        "def get_last_watermark(spark, document_type_code, target_table):\n",
        "    \"\"\"\n",
        "    Retrieve the most recent successful high_watermark for a given\n",
        "    document type and target table combination.\n",
        "    Returns a timestamp string, or '1900-01-01' if no watermark exists.\n",
        "    \"\"\"\n",
        "    # Query the processing log for the latest completed watermark\n",
        "    result = spark.sql(f\"\"\"\n",
        "        SELECT MAX(high_watermark) AS last_watermark\n",
        "        FROM {utilities_lakehouse}.clinical_processing_log\n",
        "        WHERE document_type_code = '{document_type_code}'\n",
        "          AND target_table = '{target_table}'\n",
        "          AND run_status = 'COMPLETED'\n",
        "    \"\"\").collect()\n",
        "\n",
        "    # Extract the watermark value, defaulting to epoch if none found\n",
        "    watermark = result[0][\"last_watermark\"] if result and result[0][\"last_watermark\"] else None\n",
        "\n",
        "    if watermark is None:\n",
        "        print(f\"No previous watermark found for {document_type_code}/{target_table}. Processing all rows.\")\n",
        "        return \"1900-01-01 00:00:00\"\n",
        "    else:\n",
        "        print(f\"Last watermark for {document_type_code}/{target_table}: {watermark}\")\n",
        "        return str(watermark)\n",
        "\n",
        "\n",
        "def insert_processing_log(spark, log_id, document_type_code, target_table,\n",
        "                          bronze_table_name, run_start, run_end,\n",
        "                          records_read, records_written, records_failed,\n",
        "                          high_watermark, run_status):\n",
        "    \"\"\"\n",
        "    Insert a record into clinical_processing_log to track the run\n",
        "    and store the high watermark for future incremental loads.\n",
        "    \"\"\"\n",
        "    spark.sql(f\"\"\"\n",
        "        INSERT INTO {utilities_lakehouse}.clinical_processing_log VALUES (\n",
        "            {log_id},\n",
        "            '{document_type_code}',\n",
        "            '{target_table}',\n",
        "            '{bronze_table_name}',\n",
        "            '{run_start}',\n",
        "            '{run_end}',\n",
        "            {records_read},\n",
        "            {records_written},\n",
        "            {records_failed},\n",
        "            '{high_watermark}',\n",
        "            '{run_status}'\n",
        "        )\n",
        "    \"\"\")\n",
        "    print(f\"Processing log updated: {document_type_code}/{target_table} -> {run_status}\")\n",
        "\n",
        "\n",
        "def get_next_log_id(spark):\n",
        "    \"\"\"\n",
        "    Get the next available log_id from the processing log table.\n",
        "    \"\"\"\n",
        "    return spark.sql(f\"\"\"\n",
        "        SELECT COALESCE(MAX(log_id), 0) + 1 AS next_id\n",
        "        FROM {utilities_lakehouse}.clinical_processing_log\n",
        "    \"\"\").collect()[0][\"next_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Document Type Identification\n",
        "# -------------------------------------------------------------------\n",
        "# Checks the JSON VARIANT against the clinical_document_type registry\n",
        "# to determine which document type parser to use.\n",
        "# CCDA documents are identified by the presence of a templateId entry\n",
        "# with _root matching the C-CDA R2.1 standard OID.\n",
        "\n",
        "def identify_ccda_documents(spark, bronze_df, variant_col):\n",
        "    \"\"\"\n",
        "    Filter a bronze DataFrame to only CCDA documents.\n",
        "\n",
        "    CCDA documents are identified by the presence of templateId with\n",
        "    _root = '2.16.840.1.113883.10.20.22.1.1' (C-CDA R2.1 header template).\n",
        "\n",
        "    The templateId field can be a single object or an array of objects,\n",
        "    so we check both cases.\n",
        "    \"\"\"\n",
        "    # Step 1: Load the CCDA identifier rule from the config table\n",
        "    doc_type = spark.sql(f\"\"\"\n",
        "        SELECT identifier_path, identifier_value\n",
        "        FROM {utilities_lakehouse}.clinical_document_type\n",
        "        WHERE document_type_code = 'CCDA'\n",
        "          AND is_active = true\n",
        "    \"\"\").collect()\n",
        "\n",
        "    if not doc_type:\n",
        "        print(\"WARNING: No active CCDA document type configuration found.\")\n",
        "        return bronze_df.limit(0)\n",
        "\n",
        "    identifier_value = doc_type[0][\"identifier_value\"]\n",
        "\n",
        "    # Step 2: Filter for CCDA documents\n",
        "    # Check if any templateId entry has the matching _root value.\n",
        "    # templateId can be an array or a single object, so we normalize\n",
        "    # to an array first, then check if any element matches.\n",
        "    ccda_df = bronze_df.filter(\n",
        "        expr(f\"\"\"\n",
        "            EXISTS(\n",
        "                TRANSFORM(\n",
        "                    COALESCE(\n",
        "                        TRY_CAST({variant_col}:templateId AS ARRAY<VARIANT>),\n",
        "                        ARRAY({variant_col}:templateId)\n",
        "                    ),\n",
        "                    t -> t:_root::string\n",
        "                ),\n",
        "                v -> v = '{identifier_value}'\n",
        "            )\n",
        "        \"\"\")\n",
        "    )\n",
        "\n",
        "    row_count = ccda_df.count()\n",
        "    print(f\"Identified {row_count} CCDA document(s) in the current batch.\")\n",
        "    return ccda_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727281e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Section Extraction\n",
        "# -------------------------------------------------------------------\n",
        "# CCDA documents contain up to 24 clinical sections under\n",
        "# component.structuredBody.component[]. Each section is identified\n",
        "# by a LOINC code in section.code._code.\n",
        "#\n",
        "# This function explodes the sections array, filters to the target\n",
        "# section, then normalizes and explodes the entries within it.\n",
        "# Entries can be a single object or an array, so we handle both.\n",
        "#\n",
        "# Some sections (Results, Vital Signs) have an additional nested array\n",
        "# within each entry (organizer.component[]). When sub_array_path is\n",
        "# provided, a second explosion is performed so each row represents\n",
        "# one sub-item (e.g., one observation within an organizer).\n",
        "\n",
        "def extract_section_entries(df, variant_col, section_loinc_code, sub_array_path=None):\n",
        "    \"\"\"\n",
        "    Extract entries from a specific CCDA section.\n",
        "\n",
        "    Steps:\n",
        "    1. Explode the sections array (component.structuredBody.component)\n",
        "    2. Filter to the section matching the given LOINC code\n",
        "    3. Normalize entries (handle single object vs array)\n",
        "    4. Explode entries so each row = one clinical entry\n",
        "    5. (Optional) If sub_array_path is set, explode the nested array\n",
        "       within each entry to produce one row per sub-item\n",
        "\n",
        "    Returns a DataFrame with columns:\n",
        "        bronze_id, insert_timestamp, _section_code, _section_title, _entry\n",
        "    When sub_array_path is used, _entry refers to each sub-item (e.g., each\n",
        "    component within an organizer).\n",
        "    \"\"\"\n",
        "    # Step 1: Explode the sections array from each document\n",
        "    sections_df = df.selectExpr(\n",
        "        f\"{bronze_id_column}\",\n",
        "        f\"{watermark_column}\",\n",
        "        f\"explode({variant_col}:component:structuredBody:component) as _section\"\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter to the target section by LOINC code\n",
        "    filtered_df = sections_df.filter(\n",
        "        expr(f\"_section:section:code:_code::string = '{section_loinc_code}'\")\n",
        "    )\n",
        "\n",
        "    # Step 3 & 4: Normalize entries and explode\n",
        "    # COALESCE + TRY_CAST handles both array and single-object entries:\n",
        "    #   - If entry is already an array, TRY_CAST succeeds and we use it\n",
        "    #   - If entry is a single object, TRY_CAST returns NULL, so we wrap in ARRAY()\n",
        "    entries_df = filtered_df.selectExpr(\n",
        "        f\"{bronze_id_column}\",\n",
        "        f\"{watermark_column}\",\n",
        "        f\"'{section_loinc_code}' as _section_code\",\n",
        "        f\"_section:section:title::string as _section_title\",\n",
        "        \"\"\"explode(\n",
        "            coalesce(\n",
        "                try_cast(_section:section:entry AS ARRAY<VARIANT>),\n",
        "                array(_section:section:entry)\n",
        "            )\n",
        "        ) as _entry\"\"\"\n",
        "    )\n",
        "\n",
        "    # Step 5: If a sub-array path is defined, perform a second explosion\n",
        "    # This handles sections like Results and Vital Signs where entries contain\n",
        "    # organizer.component[] arrays with multiple observations per entry.\n",
        "    if sub_array_path:\n",
        "        sub_variant_path = dot_to_variant_path(sub_array_path)\n",
        "        print(f\"Section {section_loinc_code}: exploding sub-array at '{sub_array_path}'\")\n",
        "        entries_df = entries_df.selectExpr(\n",
        "            f\"{bronze_id_column}\",\n",
        "            f\"{watermark_column}\",\n",
        "            \"_section_code\",\n",
        "            \"_section_title\",\n",
        "            f\"\"\"explode(\n",
        "                coalesce(\n",
        "                    try_cast(_entry:{sub_variant_path} AS ARRAY<VARIANT>),\n",
        "                    array(_entry:{sub_variant_path})\n",
        "                )\n",
        "            ) as _entry\"\"\"\n",
        "        )\n",
        "\n",
        "    entry_count = entries_df.count()\n",
        "    print(f\"Section {section_loinc_code}: extracted {entry_count} row(s) after all explosions.\")\n",
        "    return entries_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d2195b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Dynamic Field Extraction\n",
        "# -------------------------------------------------------------------\n",
        "# Reads field mappings from the config table and builds selectExpr\n",
        "# statements that extract values from the variant entry using\n",
        "# colon-path notation. Supports optional transformation SQL.\n",
        "#\n",
        "# Two functions are provided:\n",
        "#   build_extraction_exprs     - for section-level entry fields\n",
        "#   build_root_extraction_exprs - for document-root-level fields\n",
        "\n",
        "def build_extraction_exprs(spark, document_type_code, target_table, section_loinc_code):\n",
        "    \"\"\"\n",
        "    Build a list of SQL expressions for selectExpr() based on the\n",
        "    field mappings configured for a given target table and section.\n",
        "\n",
        "    Each mapping row becomes a selectExpr expression like:\n",
        "        _entry:path:to:field::STRING AS TARGET_COLUMN\n",
        "\n",
        "    If transformation_sql is set, it wraps the raw extraction:\n",
        "        CASE WHEN _entry:path::STRING IN (...) THEN ... END AS TARGET_COLUMN\n",
        "    \"\"\"\n",
        "    # Load active mappings for this table and section from the config\n",
        "    mappings = spark.sql(f\"\"\"\n",
        "        SELECT target_column, source_json_path, target_data_type,\n",
        "               transformation_sql, entry_sub_array_path\n",
        "        FROM {utilities_lakehouse}.clinical_field_mapping\n",
        "        WHERE document_type_code = '{document_type_code}'\n",
        "          AND target_table = '{target_table}'\n",
        "          AND section_loinc_code = '{section_loinc_code}'\n",
        "          AND path_context = 'section_entry'\n",
        "          AND is_active = true\n",
        "        ORDER BY column_ordinal\n",
        "    \"\"\").collect()\n",
        "\n",
        "    if not mappings:\n",
        "        print(f\"WARNING: No section_entry mappings found for {target_table}/{section_loinc_code}.\")\n",
        "        return []\n",
        "\n",
        "    # Build one selectExpr expression per mapping row\n",
        "    exprs = []\n",
        "    for row in mappings:\n",
        "        # Convert dot notation to variant colon notation\n",
        "        variant_path = dot_to_variant_path(row[\"source_json_path\"])\n",
        "        dtype = row[\"target_data_type\"]\n",
        "        col_name = row[\"target_column\"]\n",
        "        transform = row[\"transformation_sql\"]\n",
        "\n",
        "        # Build the raw extraction expression\n",
        "        raw_expr = f\"_entry:{variant_path}::{dtype}\"\n",
        "\n",
        "        # Apply transformation if defined, otherwise use raw extraction\n",
        "        if transform:\n",
        "            final_expr = transform.replace(\"{value}\", raw_expr) + f\" AS {col_name}\"\n",
        "        else:\n",
        "            final_expr = f\"{raw_expr} AS {col_name}\"\n",
        "\n",
        "        exprs.append(final_expr)\n",
        "\n",
        "    print(f\"Built {len(exprs)} extraction expression(s) for {target_table}/{section_loinc_code}.\")\n",
        "    return exprs\n",
        "\n",
        "\n",
        "def build_root_extraction_exprs(spark, document_type_code, target_table, variant_col):\n",
        "    \"\"\"\n",
        "    Build SQL expressions for fields extracted from the document root\n",
        "    (path_context = 'root'), not from section entries.\n",
        "\n",
        "    These are typically patient demographics and document-level identifiers\n",
        "    that apply to every entry extracted from the document.\n",
        "    \"\"\"\n",
        "    # Load active root-level mappings from the config\n",
        "    mappings = spark.sql(f\"\"\"\n",
        "        SELECT target_column, source_json_path, target_data_type, transformation_sql\n",
        "        FROM {utilities_lakehouse}.clinical_field_mapping\n",
        "        WHERE document_type_code = '{document_type_code}'\n",
        "          AND target_table = '{target_table}'\n",
        "          AND path_context = 'root'\n",
        "          AND is_active = true\n",
        "        ORDER BY column_ordinal\n",
        "    \"\"\").collect()\n",
        "\n",
        "    if not mappings:\n",
        "        print(f\"No root-level mappings found for {target_table}.\")\n",
        "        return []\n",
        "\n",
        "    # Build one selectExpr expression per root mapping\n",
        "    exprs = []\n",
        "    for row in mappings:\n",
        "        variant_path = dot_to_variant_path(row[\"source_json_path\"])\n",
        "        dtype = row[\"target_data_type\"]\n",
        "        col_name = row[\"target_column\"]\n",
        "        transform = row[\"transformation_sql\"]\n",
        "\n",
        "        # Root fields reference the variant column directly\n",
        "        raw_expr = f\"{variant_col}:{variant_path}::{dtype}\"\n",
        "\n",
        "        if transform:\n",
        "            final_expr = transform.replace(\"{value}\", raw_expr) + f\" AS {col_name}\"\n",
        "        else:\n",
        "            final_expr = f\"{raw_expr} AS {col_name}\"\n",
        "\n",
        "        exprs.append(final_expr)\n",
        "\n",
        "    print(f\"Built {len(exprs)} root extraction expression(s) for {target_table}.\")\n",
        "    return exprs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7751e58b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Main Processing Function\n",
        "# -------------------------------------------------------------------\n",
        "# Orchestrates the full bronze-to-silver extraction for a given\n",
        "# document type and target table. Handles watermarking, section\n",
        "# extraction, field mapping, and silver table writing.\n",
        "\n",
        "def process_bronze_to_silver(spark, document_type_code, target_table):\n",
        "    \"\"\"\n",
        "    End-to-end processing for one target silver table.\n",
        "\n",
        "    Steps:\n",
        "    1. Get the last watermark to determine which rows to process\n",
        "    2. Read unprocessed rows from the bronze delta table\n",
        "    3. Filter to CCDA documents using document type identification\n",
        "    4. Get the distinct section LOINC codes for this target table\n",
        "    5. For each section, extract entries and apply field mappings\n",
        "    6. Union results across sections\n",
        "    7. Join root-level fields (patient ID, org ID, etc.)\n",
        "    8. Write to the silver delta table\n",
        "    9. Update the watermark in the processing log\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Begin processing: {document_type_code} -> {target_table}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Step 1: Get last watermark\n",
        "    last_watermark = get_last_watermark(spark, document_type_code, target_table)\n",
        "\n",
        "    # Step 2: Read unprocessed rows from bronze\n",
        "    bronze_path = (\n",
        "        f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/\"\n",
        "        f\"{bronze_lakehouse}.Lakehouse/Tables/{bronze_table}\"\n",
        "    )\n",
        "    print(f\"Reading bronze table from: {bronze_path}\")\n",
        "    bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
        "\n",
        "    # Apply watermark filter to only process new rows\n",
        "    bronze_df = bronze_df.filter(col(watermark_column) > lit(last_watermark))\n",
        "\n",
        "    total_rows = bronze_df.count()\n",
        "    if total_rows == 0:\n",
        "        print(f\"No new rows to process for {target_table}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {total_rows} new row(s) to process.\")\n",
        "\n",
        "    # Capture the max watermark value for this batch\n",
        "    max_watermark = bronze_df.agg(\n",
        "        spark_max(col(watermark_column)).alias(\"max_wm\")\n",
        "    ).collect()[0][\"max_wm\"]\n",
        "\n",
        "    # Step 3: Filter to CCDA documents\n",
        "    ccda_df = identify_ccda_documents(spark, bronze_df, variant_column)\n",
        "\n",
        "    ccda_count = ccda_df.count()\n",
        "    if ccda_count == 0:\n",
        "        print(f\"No CCDA documents found in the batch. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Get distinct section codes and their sub-array paths from config\n",
        "    section_codes = spark.sql(f\"\"\"\n",
        "        SELECT DISTINCT section_loinc_code, entry_sub_array_path\n",
        "        FROM {utilities_lakehouse}.clinical_field_mapping\n",
        "        WHERE document_type_code = '{document_type_code}'\n",
        "          AND target_table = '{target_table}'\n",
        "          AND path_context = 'section_entry'\n",
        "          AND is_active = true\n",
        "          AND section_loinc_code IS NOT NULL\n",
        "    \"\"\").collect()\n",
        "\n",
        "    # Build a list of (section_code, sub_array_path) tuples\n",
        "    section_config = [\n",
        "        (row[\"section_loinc_code\"], row[\"entry_sub_array_path\"])\n",
        "        for row in section_codes\n",
        "    ]\n",
        "    print(f\"Sections to process for {target_table}: {section_config}\")\n",
        "\n",
        "    # Step 5 & 6: Extract entries from each section and union results\n",
        "    section_dfs = []\n",
        "    for code, sub_array_path in section_config:\n",
        "        print(f\"\\nExtracting from section {code} (sub_array: {sub_array_path})...\")\n",
        "\n",
        "        # Get entries for this section (explode sections, filter, explode entries)\n",
        "        # Pass sub_array_path for sections needing nested array explosion\n",
        "        entries_df = extract_section_entries(ccda_df, variant_column, code, sub_array_path)\n",
        "\n",
        "        # Build extraction expressions for this section from the mapping config\n",
        "        field_exprs = build_extraction_exprs(spark, document_type_code, target_table, code)\n",
        "\n",
        "        if not field_exprs:\n",
        "            print(f\"Skipping section {code}: no field expressions found.\")\n",
        "            continue\n",
        "\n",
        "        # Apply field extraction via selectExpr\n",
        "        # Include bronze_id and section metadata for lineage tracking\n",
        "        select_list = [\n",
        "            f\"{bronze_id_column}\",\n",
        "            f\"{watermark_column}\",\n",
        "            \"_section_code\",\n",
        "            \"_section_title\"\n",
        "        ] + field_exprs\n",
        "\n",
        "        extracted_df = entries_df.selectExpr(*select_list)\n",
        "        section_dfs.append(extracted_df)\n",
        "\n",
        "    if not section_dfs:\n",
        "        print(f\"No data extracted for {target_table}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Union all section results using unionByName\n",
        "    # allowMissingColumns=True lets sections with different field sets combine\n",
        "    # (missing columns become NULL)\n",
        "    result_df = section_dfs[0]\n",
        "    for df in section_dfs[1:]:\n",
        "        result_df = result_df.unionByName(df, allowMissingColumns=True)\n",
        "\n",
        "    print(f\"\\nCombined {result_df.count()} row(s) from all sections.\")\n",
        "\n",
        "    # Step 7: Join root-level fields to section-level data\n",
        "    root_exprs = build_root_extraction_exprs(\n",
        "        spark, document_type_code, target_table, variant_column\n",
        "    )\n",
        "\n",
        "    if root_exprs:\n",
        "        # Extract root fields once per document\n",
        "        root_df = ccda_df.selectExpr(\n",
        "            f\"{bronze_id_column}\",\n",
        "            *root_exprs\n",
        "        )\n",
        "        # Join root fields to section-level data on bronze_id\n",
        "        result_df = result_df.join(root_df, on=bronze_id_column, how=\"left\")\n",
        "\n",
        "    # Add standard audit columns and rename internal tracking fields\n",
        "    result_df = result_df \\\n",
        "        .withColumn(\"SECTION_LOINC_CODE\", col(\"_section_code\")) \\\n",
        "        .withColumn(\"BRONZE_ID\", col(bronze_id_column)) \\\n",
        "        .withColumn(\"ROW_INSERT_TIMESTAMP\", current_timestamp()) \\\n",
        "        .drop(\"_section_code\", \"_section_title\", watermark_column, bronze_id_column)\n",
        "\n",
        "    # Step 8: Write to silver delta table\n",
        "    silver_path = (\n",
        "        f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/\"\n",
        "        f\"{silver_lakehouse}.Lakehouse/Tables/{target_table.lower()}\"\n",
        "    )\n",
        "\n",
        "    records_written = result_df.count()\n",
        "    print(f\"Writing {records_written} row(s) to silver table: {target_table}\")\n",
        "    print(f\"Silver path: {silver_path}\")\n",
        "\n",
        "    result_df.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
        "    print(f\"Write complete for {target_table}.\")\n",
        "\n",
        "    # Step 9: Update watermark in the processing log\n",
        "    next_log_id = get_next_log_id(spark)\n",
        "\n",
        "    insert_processing_log(\n",
        "        spark=spark,\n",
        "        log_id=next_log_id,\n",
        "        document_type_code=document_type_code,\n",
        "        target_table=target_table,\n",
        "        bronze_table_name=bronze_table,\n",
        "        run_start=run_start,\n",
        "        run_end=datetime.now(),\n",
        "        records_read=total_rows,\n",
        "        records_written=records_written,\n",
        "        records_failed=0,\n",
        "        high_watermark=max_watermark,\n",
        "        run_status='COMPLETED'\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Completed processing: {document_type_code} -> {target_table}\")\n",
        "    print(f\"Rows read: {total_rows} | Rows written: {records_written}\")\n",
        "    print(f\"Watermark updated to: {max_watermark}\")\n",
        "    print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Execute Bronze-to-Silver Processing\n",
        "# -------------------------------------------------------------------\n",
        "# Process each target table configured for CCDA documents.\n",
        "# Each table is processed independently with its own watermark.\n",
        "# If one table fails, the others still run. Failed tables will\n",
        "# be retried on the next run since their watermark is not advanced.\n",
        "\n",
        "for table in target_tables:\n",
        "    try:\n",
        "        process_bronze_to_silver(spark, 'CCDA', table)\n",
        "    except Exception as e:\n",
        "        # Log the failure so it appears in the processing history\n",
        "        print(f\"FAILED processing {table}: {str(e)}\")\n",
        "\n",
        "        # Record the failure in the processing log\n",
        "        # The watermark is NOT advanced, so the next run will retry these rows\n",
        "        try:\n",
        "            next_log_id = get_next_log_id(spark)\n",
        "            insert_processing_log(\n",
        "                spark=spark,\n",
        "                log_id=next_log_id,\n",
        "                document_type_code='CCDA',\n",
        "                target_table=table,\n",
        "                bronze_table_name=bronze_table,\n",
        "                run_start=run_start,\n",
        "                run_end=datetime.now(),\n",
        "                records_read=0,\n",
        "                records_written=0,\n",
        "                records_failed=0,\n",
        "                high_watermark='1900-01-01 00:00:00',\n",
        "                run_status='FAILED'\n",
        "            )\n",
        "        except Exception as log_err:\n",
        "            print(f\"Additionally failed to write processing log: {str(log_err)}\")\n",
        "\n",
        "print(f\"\\nAll processing complete. Run started at {run_start}, ended at {datetime.now()}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Validation: Test extraction logic against the sample JSON\n",
        "# -------------------------------------------------------------------\n",
        "# This cell loads the sample CCDA JSON file and tests the path\n",
        "# resolution and extraction logic without writing to silver tables.\n",
        "# Uncomment and run to validate mappings during development.\n",
        "#\n",
        "# Prerequisites:\n",
        "#   - The sample JSON file must be uploaded to the lakehouse Files area\n",
        "#   - Config tables must be seeded (run ccda_config_tables.ipynb first)\n",
        "\n",
        "# sample_path = \"/lakehouse/default/Files/ccda/brnz_ccda_raw_varient.json\"\n",
        "#\n",
        "# # Load sample JSON as a single-row DataFrame with a variant-like column\n",
        "# sample_json = spark.read.text(sample_path, wholetext=True).selectExpr(\n",
        "#     \"'test-001' as bronze_id\",\n",
        "#     \"current_timestamp() as insert_timestamp\",\n",
        "#     \"parse_json(value) as raw_data\"\n",
        "# )\n",
        "# print(f\"Loaded sample document. Row count: {sample_json.count()}\")\n",
        "#\n",
        "# # Test 1: Section extraction for Results (30954-2)\n",
        "# print(\"\\n--- Testing Results Section Extraction ---\")\n",
        "# entries = extract_section_entries(sample_json, \"raw_data\", \"30954-2\")\n",
        "# print(f\"Entries found: {entries.count()}\")\n",
        "# entries.select(\"_section_code\", \"_section_title\").show(5, truncate=False)\n",
        "#\n",
        "# # Test 2: Field extraction expressions for Results\n",
        "# exprs = build_extraction_exprs(spark, \"CCDA\", \"OBSERVATION\", \"30954-2\")\n",
        "# if exprs:\n",
        "#     print(\"\\nGenerated expressions:\")\n",
        "#     for e in exprs:\n",
        "#         print(f\"  {e}\")\n",
        "#     select_list = [\"bronze_id\", \"_section_code\"] + exprs\n",
        "#     result = entries.selectExpr(*select_list)\n",
        "#     print(\"\\n--- Extracted Observation Fields ---\")\n",
        "#     result.show(5, truncate=False)\n",
        "#\n",
        "# # Test 3: Root-level extraction\n",
        "# root_exprs = build_root_extraction_exprs(spark, \"CCDA\", \"OBSERVATION\", \"raw_data\")\n",
        "# if root_exprs:\n",
        "#     print(\"\\nGenerated root expressions:\")\n",
        "#     for e in root_exprs:\n",
        "#         print(f\"  {e}\")\n",
        "#     root_result = sample_json.selectExpr(\"bronze_id\", *root_exprs)\n",
        "#     print(\"\\n--- Root-Level Fields ---\")\n",
        "#     root_result.show(truncate=False)\n",
        "#\n",
        "# # Test 4: Encounter section extraction (46240-8)\n",
        "# print(\"\\n--- Testing Encounter Section Extraction ---\")\n",
        "# enc_entries = extract_section_entries(sample_json, \"raw_data\", \"46240-8\")\n",
        "# print(f\"Encounter entries found: {enc_entries.count()}\")\n",
        "#\n",
        "# enc_exprs = build_extraction_exprs(spark, \"CCDA\", \"ENCOUNTER\", \"46240-8\")\n",
        "# if enc_exprs:\n",
        "#     enc_select = [\"bronze_id\", \"_section_code\"] + enc_exprs\n",
        "#     enc_result = enc_entries.selectExpr(*enc_select)\n",
        "#     print(\"\\n--- Extracted Encounter Fields ---\")\n",
        "#     enc_result.show(5, truncate=False)\n",
        "#\n",
        "# # Test 5: Vital Signs section extraction (8716-3)\n",
        "# print(\"\\n--- Testing Vital Signs Section Extraction ---\")\n",
        "# vs_entries = extract_section_entries(sample_json, \"raw_data\", \"8716-3\")\n",
        "# print(f\"Vital signs entries found: {vs_entries.count()}\")\n",
        "#\n",
        "# vs_exprs = build_extraction_exprs(spark, \"CCDA\", \"OBSERVATION\", \"8716-3\")\n",
        "# if vs_exprs:\n",
        "#     vs_select = [\"bronze_id\", \"_section_code\"] + vs_exprs\n",
        "#     vs_result = vs_entries.selectExpr(*vs_select)\n",
        "#     print(\"\\n--- Extracted Vital Signs Fields ---\")\n",
        "#     vs_result.show(5, truncate=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
