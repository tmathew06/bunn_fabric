{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3278ed50-842c-4749-886e-b660da4d2813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, explode, coalesce, array, current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986c54e4-31b1-4985-b08d-518f097888f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dot_to_variant_path(dot_path):\n",
    "    return dot_path.replace(\".\", \":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e58d756-348f-41fe-bfae-d6026a7dbb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_section_entries(df, variant_col, section_title, section_loinc_code=None, sub_array_path=None):\n",
    "    sections_df = df.selectExpr(\n",
    "        f\"\"\"explode(\n",
    "            coalesce(\n",
    "                try_cast({variant_col}:component:structuredBody:component AS ARRAY<VARIANT>),\n",
    "                array({variant_col}:component:structuredBody:component)\n",
    "            )\n",
    "        ) as _section\"\"\"\n",
    "    )\n",
    "\n",
    "    # 2. Filter by section code or title\n",
    "    # filtered_df = sections_df.filter(\n",
    "    #     expr(f\"_section:section:code:_code::string = '{section_loinc_code}'\")\n",
    "    # )\n",
    "    filtered_df = sections_df.filter(\n",
    "        expr(f\"_section:section:title::string = '{section_title}'\")\n",
    "    )\n",
    "    # 3. Explode the Entries inside the found section\n",
    "    entries_df = filtered_df.selectExpr(\n",
    "        f\"'{section_title}' as section_title\",\n",
    "        # Robust explode for entries\n",
    "        \"\"\"explode(\n",
    "            coalesce(\n",
    "                try_cast(_section:section:entry AS ARRAY<VARIANT>),\n",
    "                array(_section:section:entry)\n",
    "            )\n",
    "        ) as _entry\"\"\"\n",
    "    )\n",
    "\n",
    "    # 4. Optional Drill Down (e.g. into Organizer -> Component)\n",
    "    if sub_array_path:\n",
    "        sub_variant_path = dot_to_variant_path(sub_array_path)\n",
    "        entries_df = entries_df.selectExpr(\n",
    "            \"section_title\",\n",
    "            # Robust explode for sub-entries\n",
    "            f\"\"\"explode(\n",
    "                coalesce(\n",
    "                    try_cast(_entry:{sub_variant_path} AS ARRAY<VARIANT>),\n",
    "                    array(_entry:{sub_variant_path})\n",
    "                )\n",
    "            ) as _entry\"\"\"\n",
    "        )\n",
    "\n",
    "    return entries_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7177815-c780-4c64-93cd-75f4199209be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_fields(entries_df, field_mappings):\n",
    "    exprs = [\"section_title\"]\n",
    "    for target_column, source_json_path, target_data_type, transformation_sql in field_mappings:\n",
    "        variant_path = dot_to_variant_path(source_json_path)\n",
    "        \n",
    "        # Build the extraction expression (e.g., _entry:observation:id::_root::STRING)\n",
    "        raw_expr = f\"_entry:{variant_path}::{target_data_type}\"\n",
    "        \n",
    "        if transformation_sql:\n",
    "            # Apply custom SQL transform if provided\n",
    "            final_expr = transformation_sql.replace(\"{value}\", raw_expr) + f\" AS {target_column}\"\n",
    "        else:\n",
    "            final_expr = f\"{raw_expr} AS {target_column}\"\n",
    "        exprs.append(final_expr)\n",
    "        \n",
    "    return entries_df.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83212758-c5c9-43d4-9c27-51d454868f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_ccda(spark, json_string, section_title, field_mappings, sub_array_path=None):\n",
    "    # 1. Create DataFrame from raw string to prevent schema inference errors\n",
    "    data = [(json_string,)]\n",
    "    raw_df = spark.createDataFrame(data, [\"raw_json_blob\"])\n",
    "    \n",
    "    # 2. Parse into VARIANT type\n",
    "    variant_df = raw_df.selectExpr(\"parse_json(raw_json_blob) AS doc\")\n",
    "\n",
    "    # 3. Extract and Flatten\n",
    "    entries_df = extract_section_entries(variant_df, \"doc\", section_title, sub_array_path=sub_array_path)\n",
    "    result_df = extract_fields(entries_df, field_mappings)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba8d1ad-fff1-418e-805d-e8f31e501114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the file\n",
    "json_blob = open(\"/Volumes/workspace/default/files/brnz_ccda_raw_varient.json\").read()\n",
    "\n",
    "# Expanded Mappings for all Observation elements\n",
    "results_mappings = [\n",
    "  # --- Identifiers ---\n",
    "  (\"OBSERVATION_ID\",    \"observation.id._root\",  \"STRING\", None),\n",
    "  (\"OBSERVATION_MOOD_CODE\", \"observation.moodCode\", \"STRING\", None),\n",
    "  (\"OBSERVATION_CLASS_CODE\", \"observation.classCode\", \"STRING\", None),\n",
    "  \n",
    "  # --- Test Details ---\n",
    "  (\"OBSERVATION_CODE\",              \"observation.code._code\",             \"STRING\", None),\n",
    "  (\"OBSERVATION_CODE_SYSTEM\",       \"observation.code._codeSystem\",        \"STRING\", None),\n",
    "  (\"OBSERVATION_DISPLAY_NAME\",      \"observation.code._displayName\",       \"STRING\", None),\n",
    "  (\"OBSERVATION_TEST_ORIG_TEXT\",    \"observation.code.originalText\",        \"STRING\", None),\n",
    "  \n",
    "  # --- Results (Numeric vs Text) ---\n",
    "  (\"OBSERVATION_VAL_NUMERIC\",       \"observation.value.translation._value\",       \"STRING\", None),\n",
    "  (\"OBSERVATION_VAL_TEXT\",          \"observation.value._VALUE\",                   \"STRING\", None),\n",
    "  (\"OBSERVATION_UNIT\",              \"observation.value.translation.originalText\", \"STRING\", None),\n",
    "  \n",
    "  # --- Context ---\n",
    "  (\"OBSERVATION_INTERPRETATION\",    \"observation.interpretationCode._displayName\",\"STRING\", None),\n",
    "  (\"OBSERVATION_REF_RANGE\",         \"observation.referenceRange.observationRange.text\", \"STRING\", None),\n",
    "  (\"OBSERVATION_STATUS\",            \"observation.statusCode._code\",               \"STRING\", None),\n",
    "  (\"OBSERVATION_EFFECTIVE_TIME\",    \"observation.effectiveTime._value\",           \"STRING\", None),\n",
    "\n",
    "  #---------Template ID---------\n",
    "  (\"OBSERVATION_TEMPLATE_ID\",       \"observation.templateId._root\",             \"STRING\", None),\n",
    "  (\"OBSERVATION_TEMPLATE_ID_EXT\",   \"observation.templateId._extension\",        \"STRING\", None)\n",
    "  \n",
    " ]\n",
    "\n",
    "# Run the parser\n",
    "# Note: section_loinc_code is ignored in the function now, so we pass \"Results\" as the title\n",
    "df = parse_ccda(\n",
    "    spark, \n",
    "    json_blob, \n",
    "    section_title=\"Results\",\n",
    "    field_mappings=results_mappings, \n",
    "    sub_array_path=\"organizer.component\"\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ccda_parser_functions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}