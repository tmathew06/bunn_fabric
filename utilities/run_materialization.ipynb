{"cells":[{"cell_type":"code","source":["# import libraries\n","\n","from pyspark.sql import SparkSession, Row\n","from pyspark.sql.functions import col, lit, count, when, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n","from datetime import datetime\n","from delta.tables import DeltaTable\n","import os\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"b8fa049e-3950-4552-935d-91b906435247","normalized_state":"finished","queued_time":"2025-03-25T14:40:55.2611341Z","session_start_time":null,"execution_start_time":"2025-03-25T14:41:00.898593Z","execution_finish_time":"2025-03-25T14:41:01.2957391Z","parent_msg_id":"46ab7906-b350-42bb-b0bd-b35b8e3ea42b"},"text/plain":"StatementMeta(, b8fa049e-3950-4552-935d-91b906435247, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8181829a-5853-43f4-bb3a-56c7f49cd74f"},{"cell_type":"code","source":["# initialize session\n","spark = SparkSession.builder \\\n","    .appName(\"run_materialization\") \\\n","    .getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9161409Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"35c0aff6-9710-424a-a729-eadacb4f6ca2"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89bd6d73-d177-488e-9349-93efe827c96c"},{"cell_type":"markdown","source":["#### Input parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd080a02-7e6a-4818-a1fc-6a1534fce8cd"},{"cell_type":"code","source":["workspace = 'BUNN_Foundation_NONPROD'\n","lakehouse = 'silver_sapecc_lakehouse'\n","inputsourceschema = 'materialized_etl'\n","inputsource = 'orders_etl'\n","outputtargetschema = 'materialized_t'\n","outputtarget = 'orders'\n","update_control_table = 0\n","load_option = 'TR' \n","run_start = datetime.now()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.918343Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"ba0956e0-b289-46ff-a3e6-abd2369cb799"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false},"tags":["parameters"]},"id":"925f5d41-5227-4b5c-8fb0-e71ab2d767d7"},{"cell_type":"code","source":["# insert into materialization_log\n","\n","spark.sql(f\"\"\"\n","INSERT INTO materialization_log (schema_name, table_name, job_run_timestamp, run_id, run_step, run_timestamp, statement_text, lakehouse_name)\n","VALUES ('{inputsourceschema}', '{inputsource}', '{run_start}', 10, 'begin materialization', current_timestamp(), '----------', '{lakehouse}')\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9209167Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"62ebd920-acdf-48a1-bde0-2c91a4cc1a88"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b5a3fb2-d266-4fd6-b7d8-da67609625a7"},{"cell_type":"code","source":["# update control table\n","\n","if update_control_table == 1:\n","    # update materialization_control if update_control_table = 1\n","    spark.sql(f\"\"\"\n","    UPDATE materialization_control\n","    SET control_start_timestamp = current_timestamp()\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND lakehouse_name = '{lakehouse}'\n","    AND load_option = '{load_option}'\n","    \"\"\")\n","elif load_option in ('CLDUI'):\n","    # update materialization_control for specific load options\n","    spark.sql(f\"\"\"\n","    UPDATE materialization_control\n","    SET control_start_timestamp = CASE WHEN control_start_timestamp IS NULL THEN '1990-01-01 00:00:00' ELSE control_start_timestamp END\n","    WHERE etl_database = '{inputsourceschema}'\n","    AND etl_view = '{inputsource}'\n","    AND lakehouse_name = '{lakehouse}'\n","    AND load_option = '{load_option}'\n","    \"\"\")\n","else:\n","    # Insert into materialization_control if no matching record exists\n","    spark.sql(f\"\"\"\n","    INSERT INTO materialization_control (etl_database, etl_view, status_code, load_option, extract_start_timestamp, control_start_timestamp, lakehouse_name)\n","    SELECT '{inputsourceschema}', '{inputsource}', 'active', '{load_option}', '{run_start}', '{run_start}', '{lakehouse}'\n","    WHERE NOT EXISTS (\n","        SELECT 1\n","        FROM materialization_control\n","        WHERE etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    )\n","    \"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9227688Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"195d7294-629b-431b-9257-b22898863229"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cebc1052-0ad5-45ae-8e62-f3b72d92f122"},{"cell_type":"code","source":["# update materialization load table execution times\n","\n","spark.sql(f\"\"\"\n","update materialization_load\n","    set last_execution = current_timestamp\n","    WHERE 1=1\n","    and lakehouse_name = '{lakehouse}'\n","    and input_source_schema = '{inputsourceschema}'\n","    and input_source = '{inputsource}'\n","    and output_target_schema = '{outputtargetschema}'\n","    and output_target = '{outputtarget}'\n","    and load_option = '{load_option}'\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9248726Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"db9f222f-4a0a-4fb0-ba16-24687ab8148d"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ad19741-54a0-42f3-9b6f-f6dcf83a5317"},{"cell_type":"markdown","source":["#### Metadata checks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6ffc79e-f16a-4057-83b4-10f8bc62c567"},{"cell_type":"code","source":["# define input/output path\n","\n","source_table_path  = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{inputsourceschema}/{inputsource}\"\n","target_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{outputtargetschema}/{outputtarget}\"\n","log_table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/utilities_lakehouse.Lakehouse/Tables/materialization_log\"\n","\n","#print(source_schema)\n","#df = spark.read.format(\"delta\").load(output_table)\n","#df.createOrReplaceTempView(\"orders\")\n","#display(df.head(5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9269307Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"5d56d143-4c79-4af4-9a4a-4963ec4e2fcb"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"10041612-bf85-4617-bc91-1bcc6f9cc607"},{"cell_type":"code","source":["# define log schema explicitly\n","\n","log_schema = StructType([\n","    StructField(\"lakehouse_name\", StringType(), True),\n","    StructField(\"schema_name\", StringType(), True),\n","    StructField(\"table_name\", StringType(), True),\n","    StructField(\"job_run_timestamp\", TimestampType(), True),\n","    StructField(\"run_id\", IntegerType(), True), \n","    StructField(\"run_step\", StringType(), True),\n","    StructField(\"run_timestamp\", TimestampType(), True),\n","    StructField(\"record_count\", IntegerType(), True),\n","    StructField(\"step_fail\", StringType(), True),\n","    StructField(\"statement_text\", StringType(), True)\n","])\n","\n","# insert log function\n","\n","def insert_log(lakehouse_name, schema_name, table_name, job_run_timestamp,\n","             run_id, run_step, run_timestamp, record_count, step_fail, statement_text):\n","    log_data = [\n","        Row(\n","            lakehouse_name=lakehouse_name,\n","            schema_name=schema_name,\n","            table_name=table_name,\n","            job_run_timestamp=job_run_timestamp,\n","            run_id=run_id,\n","            run_step=run_step,\n","            run_timestamp=run_timestamp,\n","            record_count = record_count,\n","            step_fail=step_fail,\n","            statement_text=statement_text\n","        )\n","    ]\n","    log_df = spark.createDataFrame(log_data, schema=log_schema)\n","    log_df.write.format(\"delta\").mode(\"append\").save(log_table_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9288878Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"3b5333da-6afd-46ff-906a-b0a34211f64b"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea736ca4-1f58-407e-b5f3-3ff6af8a9b84"},{"cell_type":"code","source":["#1 Check if the source table exist\n","\n","try:\n","    if not mssparkutils.fs.exists(source_table_path):\n","         raise Exception(f\"Source table {inputsource} does not exist.\")\n","        # print(f\"Source table {inputsource} exists.\")\n","    # Log success\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=20, \n","        run_step='Success: Source object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail=None, \n","        statement_text=f\"Check table {source_table_path}\" \n","    )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=21, \n","        run_step='Failure: Source object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {source_table_path}\" \n","    )\n","    raise Exception(f\"Source table {inputsource} does not exist.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9309144Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"ddc64030-5948-48d9-8b6a-6846d4840b6c"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ca19b45-ff54-4b5f-bf07-efa45e17afad"},{"cell_type":"code","source":["#2 Check if the target table exist\n","\n","try:\n","    if not mssparkutils.fs.exists(target_table_path):\n","        raise Exception(f\"Target table {outputtarget} does not exist.\")\n","      # print(f\"Target table {outputtarget} exists.\")\n"," \n","    # success\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=30, \n","        run_step='Success: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail=None, \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=31, \n","        run_step='Failure: Target object validation', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {target_table_path}\" \n","    )\n","    raise Exception(f\"Target table {outputtarget} does not exist.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9328295Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"ab0bc2a1-477b-414a-8e96-d2b17b5577a6"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d0ad13e-7e72-477d-8adb-2c09f7a5ab1c"},{"cell_type":"code","source":["#3 source and traget columns check\n","\n","try:\n","    source_df = spark.read.format(\"delta\").load(source_table_path)\n","    target_df = spark.read.format(\"delta\").load(target_table_path)\n","\n","    source_column_count = len(source_df.columns)\n","    target_column_count = len(target_df.columns)\n","\n","    # print(f\"Source table column count: {source_column_count}\")\n","    # print(f\"Target table column count: {target_column_count}\")\n","\n","    if not source_column_count == target_column_count:\n","        raise Exception(f\"Missing columns in source/target object\")\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,  \n","        table_name=inputsource,  \n","        job_run_timestamp=run_start,  \n","        run_id=40, \n","        run_step='Success: Column counts match', \n","        run_timestamp=datetime.now(), \n","        record_count = None,\n","        step_fail= None, \n","        statement_text=f\"Check table {inputsource}\" \n","        )\n","except Exception as e:\n","    # failure\n","    insert_log(\n","        lakehouse_name=lakehouse, \n","        schema_name=inputsourceschema, \n","        table_name=inputsource, \n","        job_run_timestamp=run_start, \n","        run_id=41, \n","        run_step='Failure: Missing column(s) in source/target object', \n","        run_timestamp=datetime.now(), \n","        record_count = 1,\n","        step_fail=True,  \n","        statement_text=f\"Check table {inputsource}\" \n","    )\n","    raise Exception(f\"Missing column(s) in source/target object.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9347228Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"fa2f9af6-2e3e-45e9-b636-0a31f6ff3902"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"8681edec-6960-4524-af0c-d96335d41918"},{"cell_type":"code","source":["# Print source table schema\n","source_df = spark.read.format(\"delta\").load(source_table_path)\n","source_df.printSchema()\n","\n","# Print target table schema\n","target_df = spark.read.format(\"delta\").load(target_table_path)\n","target_df.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"a8238ed8-fe46-4f89-9ca1-f0bcf720e831"},{"cell_type":"markdown","source":["#### Data loads"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e73a4428-558f-491e-a75a-2c7424d8de7f"},{"cell_type":"code","source":["1# Truncate and Reload (TR)\n","\n","if load_option == \"TR\":\n","    try:\n","        try:\n","            # Check if the path of the Delta table exists\n","            if DeltaTable.isDeltaTable(spark, target_table_path):\n","                # Load the Delta table\n","                delta_table = DeltaTable.forPath(spark, target_table_path)\n","\n","                # Delete all rows from the target\n","                delta_table.delete(\"1 = 1\") \n","                print(\"Truncate operation completed successfully\")             \n","\n","                # Log success for truncate\n","                insert_log(\n","                    lakehouse_name=lakehouse,\n","                    schema_name=inputsourceschema,\n","                    table_name=inputsource,\n","                    job_run_timestamp=run_start,\n","                    run_id=101, \n","                    run_step=\"Success: Truncate target object\",\n","                    run_timestamp=datetime.now(),\n","                    record_count=None,\n","                    step_fail=None,\n","                    statement_text=None\n","                )  \n","            else:\n","                raise Exception(f\"Delta table not found at path {target_table_path}\")\n","\n","        except Exception as truncate_error:\n","            # failure for truncate\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=191,\n","                run_step=\"Execution Error: Truncate target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(truncate_error)}\"\n","            )\n","            print(f\"FAIL (190) Truncate failed. Error - {str(truncate_error)}\")\n","            raise truncate_error\n","\n","        try:\n","            \n","            input_df = spark.read.format(\"delta\").load(source_table_path)\n","            input_df.write.format(\"delta\").mode(\"append\").save(target_table_path)\n","\n","            # successful load\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=102,\n","                run_step=\"Success: Load target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=input_df.count(),\n","                step_fail=None,\n","                statement_text=None\n","            )\n","\n","        except Exception as load_error:\n","            # failure for load\n","            insert_log(\n","                lakehouse_name=lakehouse,\n","                schema_name=inputsourceschema,\n","                table_name=inputsource,\n","                job_run_timestamp=run_start,\n","                run_id=192, \n","                run_step=\"Execution Error: Load target object\",\n","                run_timestamp=datetime.now(),\n","                record_count=1,\n","                step_fail=\"True\",\n","                statement_text=f\"Error - {str(load_error)}\"\n","            )\n","            print(f\"FAIL (191) TR load failed. Error - {str(load_error)}\")\n","            raise load_error\n","\n","    except Exception as general_error:\n","        # general failure\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=199,\n","            run_step=\"Execution Error: General failure in TR load process\",\n","            run_timestamp=datetime.now(),\n","            record_count=1,\n","            step_fail=\"True\",\n","            statement_text=f\"Error - {str(general_error)}\"\n","        )\n","        print(f\"FAIL (199) General failure in TR load process. Error - {str(general_error)}\")\n","        raise general_error\n","else:\n","    try:\n","        raise ValueError(f\" Unsupported load_option - {load_option}\")\n","    except Exception as e:\n","        # failure for unsupported load_option\n","        insert_log(\n","            lakehouse_name=lakehouse,\n","            schema_name=inputsourceschema,\n","            table_name=inputsource,\n","            job_run_timestamp=run_start,\n","            run_id=999,  \n","            run_step=\"Execution Error: Unsupported load_option\",\n","            run_timestamp=datetime.now(),\n","            record_count=1,\n","            step_fail=\"True\",\n","            statement_text=f\"Error - {str(e)}\"\n","        )\n","        print(f\"FAIL (999) Unsupported load_option. Error - {str(e)}\")\n","        raise e        "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9368791Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"69ceca99-7d30-45cb-8644-805a01861e8c"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78c21c8f-baab-4112-b7a4-763a487f6b09"},{"cell_type":"code","source":["# Load complete check\n","\n","if load_option in [\"TR\"]:\n","    insert_log(\n","        lakehouse_name=lakehouse,\n","        schema_name=inputsourceschema,\n","        table_name=inputsource,\n","        job_run_timestamp=run_start,\n","        run_id=200, \n","        run_step=\"Load complete\",\n","        run_timestamp=datetime.now(),\n","        record_count=None,\n","        step_fail=None,\n","        statement_text=None\n","    )\n","\n","    # Update materialization_control table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_control\n","        SET \n","            extract_start_timestamp = '{run_start}',\n","            extract_end_timestamp = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND etl_database = '{inputsourceschema}'\n","        AND etl_view = '{inputsource}'\n","        AND load_option = '{load_option}'\n","        AND lakehouse_name = '{lakehouse}'\n","    \"\"\")\n","\n","    # update materialization_load table\n","    spark.sql(f\"\"\"\n","        UPDATE materialization_load\n","        SET\n","            last_successful_execution = '{datetime.now()}'\n","        WHERE 1 = 1\n","        AND lakehouse_name = '{lakehouse}'\n","        AND input_source_schema = '{inputsourceschema}'\n","        AND input_source = '{inputsource}'\n","        AND output_target_schema = '{outputtargetschema}'\n","        AND output_target = '{outputtarget}'\n","        AND load_option = '{load_option}'\n","    \"\"\")\n","\n","    print(f\"Load completed successfully for load_option: {load_option}.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9387525Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"15dcdd62-8066-4e81-a93e-1f4254b1a891"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f74b498f-29bf-4de0-a347-0777ee9f4e7a"},{"cell_type":"code","source":["# stop spark session\n","spark.stop()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-03-25T14:30:48.9406797Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"6edd7bc3-8960-430d-a709-0f2325dbd711"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"21edc9a4-a398-4c1a-a16c-40e089358ee6"},{"cell_type":"markdown","source":["#### Validate results"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61818c32-4d0f-4bb7-a86e-52535f6bc2d8"},{"cell_type":"code","source":["# Verify the results in materialization_log\n","display(spark.sql(f\"\"\"\n","SELECT * FROM materialization_log\n","WHERE schema_name = '{inputsourceschema}' AND table_name = '{inputsource}'\n","\"\"\"))\n","\n","# Verify the results in materialization_control\n","display(spark.sql(f\"\"\"\n","SELECT * FROM materialization_control\n","WHERE etl_database = '{inputsourceschema}' AND etl_view = '{inputsource}'\n","\"\"\"))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"7b0f5410-b8fb-4aaf-92ea-1cfe6ebe16e8"},{"cell_type":"code","source":["%%sql\n","select * from materialization_load where 1=1 and lakehouse_name ='silver_sapecc_lakehouse'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"7d0abca0-cc52-4a6c-bf68-e7fa4f65bc03"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d0ace1d4-3de2-4379-9bea-a092648d9ae3","default_lakehouse_name":"utilities_lakehouse","default_lakehouse_workspace_id":"45996e9c-972f-49c2-89b8-4f3f04269611","known_lakehouses":[{"id":"d0ace1d4-3de2-4379-9bea-a092648d9ae3"},{"id":"b70a9a68-ce81-4761-86b7-5005c5e65fed"},{"id":"f0b6b768-1033-4592-962a-c84a93795bd4"}]}}},"nbformat":4,"nbformat_minor":5}